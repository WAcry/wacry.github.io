---
title: "Глубокий анализ теоремы CAP: создание высокопараллельных и высокодоступных распределенных систем"
date: 2024-12-27
draft: false
description: "Обсуждение применения теоремы CAP в распределенных системах от теории к практике."
summary: "Обсуждение применения теоремы CAP в распределенных системах от теории к практике."
tags: [ "распределенные системы", "теорема CAP", "проектирование систем", "модели согласованности" ]
categories: [ "проектирование систем" , "распределенные системы" ]
---

## I. Теорема CAP

### 1.1 Что такое теорема CAP

**Теорема CAP** была предложена Эриком Брюером в 2000 году, и ее основная идея заключается в следующем:

- **C (Consistency, согласованность)**: все узлы в системе видят одни и те же данные в один и тот же момент времени. Строго говоря, когда клиент считывает данные, независимо от того, из какой реплики он их считывает, результат должен соответствовать последним отправленным данным (обычно подразумевается строгая согласованность/линейная согласованность).
- **A (Availability, доступность)**: система может продолжать предоставлять нормальные услуги, даже если возникают частичные сбои, и каждый запрос может получить "действительный ответ" в разумное время (не обязательно успешный, но и правильный ответ об ошибке).
- **P (Partition tolerance, отказоустойчивость к разделению)**: система может выдерживать разделение сети (недоступность связи между узлами), и даже если сеть разделяется, система может обеспечить определенную степень доступности или согласованности.

В реальной распределенной среде разделение сети неизбежно, поэтому **P** в основном считается "обязательным вариантом". Когда происходит разделение сети, система не может одновременно учитывать **строгую согласованность** данных и **высокую доступность** для всех узлов, поэтому приходится выбирать между C и A, что приводит к двум основным типам: **CP** и **AP**.

### 1.2 Ограничения теоремы CAP

Следует отметить, что теорема CAP сама по себе является относительно высокоуровневой теорией, которая используется для концептуального руководства, и **не следует ее понимать как "либо выбираем C, либо выбираем A"**. Существует ряд распространенных заблуждений:

1. **C не обязательно означает строгую согласованность**
   C в теореме CAP часто относится к согласованности в самом строгом смысле (т.е. линейной согласованности). Однако в реальных системах у нас есть множество моделей на выбор, таких как слабая согласованность, чтение подтвержденных данных (Read Committed), причинная согласованность (Causal Consistency) и т.д.
2. **Доступность не является 0 или 1**
   Нельзя сказать, что если выбран CP, то доступность полностью принесена в жертву; или если выбран AP, то согласованность ничем не гарантируется. И доступность, и согласованность имеют разные степени компромисса и стратегии снижения.
3. **Итоговая согласованность** не противоречит CAP
   Это очень распространенный компромисс, который использует более низкую согласованность записи для получения более высокой доступности и пропускной способности, а также асинхронно сходится к данным в фоновом режиме.

Таким образом, теорема CAP должна сочетаться с различными **моделями согласованности** и **архитектурными моделями высокой доступности** в конкретных сценариях, чтобы создать реальную практическую ценность.

------

## II. Модели согласованности в распределенных системах

Классификация моделей согласованности очень богата, но основные распространенные модели можно в целом разделить на: **строгую согласованность** и **слабую согласованность** (которая включает итоговую согласованность, причинную согласованность и т.д.). В этой статье в основном представлены **строгая согласованность** и **итоговая согласованность**, а также объясняется их распространенное применение в режимах CP или AP.

### 2.1 Строгая согласованность

**Строгая согласованность (Strong Consistency)**, также известная как **линейная согласованность (Linearizability)**, означает, что как только операция записи завершается и возвращает успех, любая последующая операция чтения может прочитать это обновление. Другими словами, система ведет себя так, как будто все операции выполняются последовательно.

- **Распространенная реализация**: зависит от синхронной репликации и механизма кворума (большинства), используя протоколы (такие как Paxos/Raft) для обеспечения наличия только одного эффективного лидера (Leader) в системе, все операции записываются в журнал в порядке и реплицируются на большинство узлов.
- Преимущества и недостатки:
    - Преимущества: гарантирует строгую правильность данных, данные, прочитанные в любое время, не "откатываются".
    - Недостатки: при колебаниях сети, разделении или сбое лидера, для поддержания согласованности часто блокируются операции записи, что приводит к снижению общей доступности; производительность и пропускная способность также относительно ниже.

### 2.2 Итоговая согласованность

**Итоговая согласованность (Eventual Consistency)** является типичной формой слабой согласованности, она требует только, чтобы если в системе больше нет новых операций обновления, со временем данные всех реплик постепенно сходились к одному и тому же состоянию. В течение этого периода пользователи, считывающие данные реплики, могут видеть устаревшие значения, но в конечном итоге они станут согласованными.

- **Распространенная реализация**: протокол Gossip, асинхронная репликация нескольких реплик, CRDT (Conflict-free Replicated Data Type) и т.д.
- Преимущества и недостатки:
    - Преимущества: высокая доступность, высокая пропускная способность, низкая задержка операций записи, высокая устойчивость к разделению сети.
    - Недостатки: необходимо допускать кратковременную несогласованность данных, более сложную логику приложения, может потребоваться обнаружение и слияние конфликтов.

------

## III. Распространенные протоколы и алгоритмы согласованности

Чтобы обеспечить согласованность между репликами распределенной системы, в отрасли было предложено множество классических алгоритмов и протоколов. Ниже кратко представлены некоторые из них:

### 3.1 Paxos

Paxos - это алгоритм распределенной согласованности, предложенный Лесли Лэмпортом в 1990-х годах, который в основном используется для реализации строгой или линейной согласованности.

- **Основной принцип**: путем разделения ролей (предлагающий Proposer, принимающий Acceptor, обучающийся Learner) проводится многораундовое голосование, чтобы определить, принята ли операция или значение большинством узлов.
- Преимущества и недостатки:
    - Преимущества: может достичь согласованности при разделении сети и сбоях узлов, обладает высокой безопасностью.
    - Недостатки: сложная реализация, высокая сложность отладки и устранения неполадок, многораундовое голосование приводит к ограничению производительности. В промышленности в основном используются его варианты (Multi-Paxos и т.д.).

### 3.2 Raft

Raft был официально предложен в 2013 году с целью **упростить реализацию и понимание при сохранении той же безопасности, что и Paxos**. Он устанавливает стабильную роль **лидера (Leader)**, централизованно выполняя репликацию журналов и восстановление после сбоев:

- **Ключевые этапы**: выборы лидера (Leader Election), репликация журналов (Log Replication), безопасность (Safety) и т.д.
- **Распространенные приложения**: Etcd, Consul, TiKV, LogCabin и т.д. основаны на Raft для реализации строгой согласованной репликации.
- Преимущества и недостатки:
    - Преимущества: относительно прост для понимания, меньший объем кода реализации; хорошая производительность для небольших и средних кластеров.
    - Недостатки: зависит от основного узла (лидера), сбой или разделение основного узла вызовет кратковременную блокировку записи; при крупномасштабных кластерах или межрегиональном развертывании задержка и доступность будут затронуты.

### 3.3 Протокол Gossip

Протокол Gossip (сплетни) не является традиционным протоколом консенсуса, он в основном используется в децентрализованных сценариях для обмена метаданными или информацией о состоянии путем случайного взаимодействия узлов, чтобы распространять и сходиться по всей сети.

- **Особенности**: децентрализация, низкие накладные расходы, периодический и случайный обмен сообщениями между узлами.
- **Распространенные приложения**: Cassandra, Riak, распределенное управление участниками (например, Serf) и т.д., используются для реализации итоговой согласованности, синхронизации состояния реплик и т.д.
- Преимущества и недостатки:
    - Преимущества: хорошая масштабируемость, простая реализация, подходит для сценариев, где требования к согласованности невысоки, а требования к масштабируемости высоки.
    - Недостатки: слабая гарантия согласованности, требуются более продвинутые средства обработки конфликтов (такие как CRDT, слияние номеров версий и т.д.) для окончательного разрешения конфликтов.

### 3.4 2PC / 3PC

В сценариях распределенных транзакций распространенными протоколами фиксации являются **2PC (Two-phase Commit)** и **3PC (Three-phase Commit)**:

- **2PC**: координатор уведомляет всех участников о "подготовке (prepare)", если все успешно, то транслирует "фиксацию (commit)", в противном случае "откат (abort)".
- **3PC**: добавляет этап на основе 2PC, чтобы уменьшить блокировку, вызванную единичной точкой отказа, но реализация более сложная, и все еще существуют проблемы с недоступностью в экстремальных сценариях разделения сети или сбоев.
- Преимущества и недостатки:
    - Преимущества: легко понять, четкая семантика транзакций, широко используется в распределенных базах данных, очередях сообщений и т.д.
    - Недостатки: сильная зависимость от координатора, риск блокировки; транзакция может не продвигаться, если сеть разделена на длительное время.

------

## IV. Два основных варианта выбора CAP: CP и AP

После того, как мы определили, что **P** является "обязательным" атрибутом, если распределенная система хочет продолжать предоставлять услуги во время разделения сети, она должна сделать выбор между **C** и **A**. Поэтому распространенные конструкции систем разделяются на два основных лагеря: **CP** и **AP**.

### 4.1 CP-системы

**CP (Consistency + Partition tolerance)**: при возникновении разделения сети система выберет **приоритет обеспечения согласованности**, при необходимости **жертвуя доступностью**.

- Типичная реализация:
    - Консенсус большинства (Paxos, Raft и т.д.), требуется, чтобы более половины узлов были активны и достигли согласия, прежде чем разрешить запись.
    - Если в данный момент не удается достичь кворума (quorum) или происходит сбой основного узла, система будет блокировать или отклонять операции записи, чтобы предотвратить расщепление мозга, приводящее к несогласованности данных.
- Распространенные приложения:
    - Zookeeper, Etcd, Consul, службы распределенной блокировки, распределенное управление метаданными и т.д.
    - Основные процессы финансовых транзакций, банковские системы учета и другие сценарии с высокими требованиями к согласованности.
- Особенности:
    - Имеет строгую гарантию данных: лучше отключиться, чем иметь двойной основной или путаницу данных.
    - Жертвует определенной доступностью: при возникновении разделения сети или переключении при сбое будет окно, когда служба недоступна или отклоняет операции записи.

### 4.2 AP-системы

**AP (Availability + Partition tolerance)**: при возникновении разделения сети система выберет **приоритет обеспечения доступности**, при этом **ослабляя согласованность**.

- Типичная реализация:
    - Итоговая согласованность, репликация нескольких основных узлов, протокол Gossip, настраиваемая политика согласованности в стиле Dynamo и т.д.
- Распространенные приложения:
    - Базы данных NoSQL (Cassandra, Riak, DynamoDB и т.д.), распределенные системы кэширования (Redis Cluster) и т.д.
    - Социальные сети, сбор журналов, системы рекомендаций и другие виды бизнеса, которые требуют высокой доступности, высокой пропускной способности и относительно низких требований к согласованности данных.
- Особенности:
    - Даже при разделении все узлы по-прежнему принимают запросы на чтение и запись, гарантируя, что система "максимально доступна".
    - Данные могут иметь кратковременную несогласованность, но будут постепенно сходиться в фоновом режиме посредством асинхронной синхронизации, слияния конфликтов и т.д.

------

## V. Как сделать выбор между CP и AP?

В реальных крупномасштабных распределенных системах **редко полагаются только на одну модель**, а скорее обрабатывают различные данные или бизнес-сценарии по уровням, чтобы добиться оптимального баланса между **согласованностью** и **доступностью**.

1. **Для основных данных выбирайте CP**
    - Например, баланс счета пользователя, оплата заказа, поток финансовых транзакций и т.д., которые имеют очень высокие требования к согласованности.
    - Допускается кратковременная невозможность записи, вызванная колебаниями сети, но нельзя допускать ошибок в балансе или сумме транзакции.
2. **Для периферийных или кэшированных данных выбирайте AP**
    - Например, кэш страницы сведений о товаре, журналы поведения пользователей, списки кандидатов на рекомендации и т.д., которые имеют более низкие требования к согласованности.
    - Больше внимания уделяется высокой параллельности и высокой доступности, допускается определенная задержка обновления или грязное чтение.

Многие интернет-компании используют **гибридную архитектуру**: основные процессы транзакций используют хранилище в стиле CP (например, распределенные реляционные базы данных или распределенное хранилище со строгой согласованностью); периферийные бизнес-сценарии или сценарии "чтения больше, чем записи" используют хранилище в стиле AP или решения для кэширования.

------

## VI. Как CP и AP реализуют высокую параллельность и итоговую согласованность

### 6.1 Как CP-системы справляются с высокой параллельностью

Хотя протоколы консенсуса сталкиваются с более высокой задержкой и более низкой пропускной способностью, когда размер одного кластерного узла и количество запросов на запись велики, они все же могут улучшить параллелизм и масштабируемость следующими способами:

1. Пакетное чтение и запись
    - Упакуйте несколько операций записи на стороне клиента или на промежуточном уровне и запишите их на узел лидера за один раз, чтобы уменьшить количество сетевых переходов и раундов протокола.
2. Разделение базы данных и таблиц и несколько кластеров
    - Разделите данные на несколько кластеров (шардинг) по логике или хешу, каждый кластер по-прежнему запускает протокол CP; запросы распределяются по разным сегментам через уровень маршрутизации или прокси.
    - Улучшите общую параллельную способность и ограничьте влияние сбоев в пределах одного сегмента.

> Пропускная способность односегментного кластера CP-систем часто в 2-10 раз ниже, чем у AP-систем.

### 6.2 Как AP-системы обеспечивают итоговую согласованность

AP-системы обычно могут обеспечить высокую пропускную способность записи и доступность чтения, но они ослабляют согласованность, поэтому необходимо реализовать гарантию сходимости согласованности в фоновом режиме или на уровне бизнес-логики:

1. Номер версии (Vector Clock) или логическая метка времени
    - Назначьте номер версии (или на основе Lamport Clock / Hybrid Clock) каждой операции обновления, объедините в сценариях конфликтов или используйте стратегию выигрыша на основе метки времени (Last Write Wins).
2. Протокол Gossip / механизм антиэнтропии (Anti-entropy)
    - Узлы периодически обмениваются последними данными или метаданными, и если обнаруживаются конфликты, они объединяются.
3. Настраиваемая политика согласованности
    - Представленная моделью Dynamo, клиент может настроить такие параметры, как `R`, `W` (например, запись большинства, подтверждение реплики), чтобы гибко регулировать согласованность и доступность.
4. Пользовательская стратегия разрешения конфликтов
    - Объедините с бизнес-семантикой, например, объедините корзину покупок с помощью "объединения", а счетчик использует CRDT (G-counter, PN-counter и т.д.) для обеспечения монотонности данных.

------

## VII. Реализация строгой согласованности между сегментами CP

Как упоминалось в главе VII, **разделение базы данных и таблиц (Sharding)** может "разделить" давление одного кластера CP на несколько подкластеров для поддержки более высокой параллельности. Однако, когда бизнесу необходимо выполнять транзакции между сегментами (т.е. обновление нескольких баз данных или таблиц), он по-прежнему сталкивается с проблемой **согласованности нескольких сегментов**. Обычно есть следующие идеи:

1. **Распределенные транзакции: 2PC / 3PC**
    - Если приложению требуется атомарное обновление нескольких сегментов, обычно используются протоколы распределенных транзакций (например, 2PC, 3PC) для координации фиксации или отката каждого сегмента.
    - Проблемы и контрмеры:
        - 2PC/3PC зависят от узла координатора, который может стать узким местом.
        - В экстремальных ситуациях, когда сеть сильно разделена или координатор выходит из строя, может возникнуть блокировка.
        - Обычно используются переключение основного и резервного, обнаружение пульса и механизмы тайм-аута, идемпотентные повторные попытки, MVCC и т.д. для снижения влияния блокировки и риска несогласованности данных.
2. **Архитектура на основе ячеек (Cell-based)**
    - Разделите бизнес на несколько автономных ячеек, данные в каждой ячейке находятся в одном наборе сегментов, чтобы гарантировать, что большинство транзакций выполняются только в одной ячейке, уменьшая количество операций между сегментами.
    - Используйте асинхронные или итоговые механизмы согласованности на границах ячеек для обмена данными, учитывая общую высокую доступность и согласованность.
3. **Глобальная распределенная база данных + глобальный протокол консенсуса**
    - Например, Google Spanner реализует строгую согласованную репликацию реплик через Paxos в каждом сегменте (Shard), а затем использует TrueTime API для предоставления глобальных меток времени для обеспечения согласованности между сегментами.
    - Этот план имеет чрезвычайно высокую сложность реализации, но может обеспечить возможности распределенных транзакций, близкие к строгой согласованности в глобальном масштабе.

> **Резюме**: для межсегментных транзакций, которые строго требуют строгой согласованности, **2PC/3PC + координатор** по-прежнему являются распространенными решениями, и путем максимально возможного повышения высокой доступности координатора снижается вероятность сбоя. Но в инженерной практике следует стараться максимально сократить операции записи между сегментами или использовать идею ячеек, чтобы ограничить большинство транзакций в пределах одного сегмента, чтобы снизить сложность системы.

------

## VIII. Обсуждение известных случаев

Ниже кратко рассмотрим несколько распределенных систем, которые часто упоминаются в отрасли, и посмотрим на их компромиссы и методы реализации в CAP:

1. **Google Spanner**
    - Типичная **CP**-система (даже может достичь "иллюзии CA", о которой часто говорят внешние источники, но по сути все равно нужно пожертвовать частью доступности).
    - Использует точные внешние метки времени, предоставляемые TrueTime + репликацию Paxos внутри каждого сегмента, чтобы гарантировать строгую согласованность между центрами обработки данных.
    - Подходит для глобальных финансовых транзакций или сценариев с высокими требованиями к согласованности, но стоимость инфраструктуры чрезвычайно высока.
2. **BigTable / HBase**
    - На первый взгляд больше склоняется к **CP**, согласованность метаданных обеспечивается через распределенную координацию между RegionServer и Master.
    - Но на самом деле путь чтения и записи также может обеспечить определенные средства высокой доступности за счет асинхронной репликации нескольких реплик, а согласованность чтения может быть скорректирована в соответствии с потребностями приложения.
3. **AWS DynamoDB**
    - Склоняется к **AP**, ранний дизайн был вдохновлен статьей Dynamo, и уровень согласованности можно регулировать с помощью таких параметров, как `R`, `W`.
    - В режиме по умолчанию обеспечивает очень высокую доступность и итоговую согласованность, а также может включить "строгое согласованное чтение" (но гарантирует только строгую согласованность одного сегмента, не обязательно между сегментами).
4. **Cassandra**
    - Также склоняется к **AP**, в основе используется протокол Gossip для поддержания состояния топологии узлов.
    - Согласованность чтения и записи может быть настроена с помощью количества реплик чтения и записи `R` / `W`, чтобы обеспечить плавный переход от итоговой согласованности к более строгой согласованности.

> **Сравнение показывает**: в инженерии не существует абсолютного "AP или CP", скорее это смесь различных стратегий согласованности; большинство систем обеспечивают определенную степень настраиваемой согласованности для адаптации к различным сценариям применения.

------

## IX. Заключение

1. **Теорема CAP не является универсальным решением**
    - В реальных распределенных системах нельзя просто сказать: "Я выбираю C и отказываюсь от A" или "Я выбираю A и отказываюсь от C".
    - В отрасли чаще встречается гибкий выбор режима **CP** или **AP** для разных измерений данных, разных типов операций, и даже в одной системе используются разные стратегии отказоустойчивости и согласованности для разных таблиц/разных функций.
2. **AP не является абсолютно 100% доступным**
    - Например, Cassandra, DynamoDB и т.д. также могут не удовлетворять запросы в экстремальных ситуациях разделения сети или массового отказа узлов.
    - AP-системы просто спроектированы так, чтобы "записывать, пока реплика доступна для записи", жертвуя частью гарантии согласованности в обмен на относительно более высокую доступность и пропускную способность.
3. **CP также может стараться обеспечить высокую доступность**
    - Paxos/Raft также могут обеспечить доступность 99,99% или даже выше в нормальных условиях, но требуется вложить больше сетевых, аппаратных и инженерных затрат, и при экстремальном разделении сети все равно будет блокировка записи, жертвуя доступностью для поддержания согласованности.
4. **Гибридная архитектура является основной**
    - Основные сценарии транзакций настаивают на строгой согласованности (CP), а периферийные вспомогательные сценарии или каналы кэширования используют слабую согласованность (AP), и они работают вместе.
    - Необходимо учитывать толерантность бизнеса, сетевую среду, вложения в затраты и технические резервы команды для всестороннего компромисса.

Теорема CAP обеспечивает высокоуровневую основу для проектирования распределенных систем, помогая нам принимать рациональные решения перед лицом неизбежной реальности разделения сети. В реальных системах необходимо использовать более богатые **модели согласованности**, **протоколы консенсуса**, **механизмы репликации нескольких реплик**, а также инженерную практику (аварийное восстановление, понижение, идемпотентность, слияние конфликтов и т.д.) для балансировки согласованности и доступности.