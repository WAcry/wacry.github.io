---
title: "CAP定理の徹底解説：高並行性と高可用性を備えた分散システムを構築する"
date: 2024-12-27
draft: false
description: "分散システムにおけるCAP定理の応用を理論から実践まで議論します。"
summary: "分散システムにおけるCAP定理の応用を理論から実践まで議論します。"
tags: [ "分散システム", "CAP定理", "システム設計", "一貫性モデル" ]
categories: [ "システム設計" , "分散システム" ]
---

## 一、CAP定理

### 1.1 CAP定理とは

**CAP定理**は、2000年にEric Brewerによって提唱されたもので、その核心的な考え方は以下のとおりです。

- **C（Consistency、一貫性）**：システム内のすべてのノードが、同じ瞬間に同じデータを見ている状態を指します。より厳密に言えば、クライアントがデータを読み取る際、どのレプリカから読み取っても、結果は最新のコミットされたデータと一致している必要があります（通常、強い一貫性/線形一貫性を指します）。
- **A（Availability、可用性）**：システムが部分的な障害が発生した場合でも、正常なサービスを提供し続けることができる状態を指します。すべてのリクエストが妥当な時間内に「有効な応答」（必ずしも成功だけでなく、正しい失敗応答も含む）を得られる必要があります。
- **P（Partition tolerance、分断耐性）**：システムがネットワーク分断（ノード間の通信が到達不能になる）を許容できる状態を指します。ネットワークが分断された場合でも、システムはある程度の可用性または一貫性を提供できる必要があります。

実際の分散環境では、ネットワーク分断は避けられないため、**P**は基本的に「必須」と見なされます。ネットワーク分断が発生した場合、システムはすべてのノードのデータの**強い一貫性**と**高可用性**を同時に両立させることはできず、CとAのどちらかを選択する必要があります。これにより、**CP**と**AP**という2つの主要なタイプが派生しました。

### 1.2 CAP定理の限界

注意すべき点は、CAP定理自体が比較的高レベルの理論であり、概念的な指針として適用されるべきであり、**「Cを選ぶか、Aを選ぶか」と単純に理解すべきではない**ということです。以下のような一般的な誤解があります。

1. **Cは必ずしも強い一貫性ではない**
   CAP定理におけるCは、最も厳密な意味での一貫性（すなわち線形一貫性）を指すことが多いです。しかし、実際のシステムでは、弱い一貫性、読み取りコミット（Read
   Committed）、因果一貫性（Causal Consistency）など、多くの細粒度モデルを選択できます。
2. **可用性は0か1ではない**
   CPを選んだからといって、可用性が完全に犠牲になるわけではありません。また、APを選んだからといって、一貫性が全く保証されないわけでもありません。可用性と一貫性には、さまざまな程度のトレードオフとダウングレード戦略が存在します。
3. **結果整合性**はCAPに違反しない
   これは非常に一般的な妥協案であり、低い書き込み一貫性と引き換えに、より高い可用性とスループットを実現し、バックグラウンドで非同期的にデータを収束させます。

したがって、CAP定理は、さまざまな**一貫性モデル**や**高可用性アーキテクチャパターン**と具体的なシナリオを組み合わせて初めて、真の具体的な指導価値を生み出すことができます。

------

## 二、分散システムの一貫性モデル

一貫性モデルの分類は非常に豊富ですが、一般的な主流モデルは、**強い一貫性**と**弱い一貫性**（結果整合性、因果一貫性などを含む）に大別できます。この記事では、主に**強い一貫性**と**結果整合性**を紹介し、それらがCPまたはAPモードでどのように一般的に使用されるかを説明します。

### 2.1 強い一貫性

**強い一貫性（Strong Consistency）**は、**線形一貫性（Linearizability）**とも呼ばれ、一度書き込み操作が完了して成功が返された場合、その後のすべての読み取り操作でその更新内容を読み取ることができる状態を指します。つまり、システムはすべての操作をシリアルに実行したかのように外部に振る舞います。

- **一般的な実装**：同期レプリケーションと仲裁（多数派）メカニズムに依存し、プロトコル（Paxos/Raftなど）を通じて、システム内に有効なリーダー（Leader）が1つだけであることを保証します。すべての操作はログに順番に書き込まれ、多数のノードに複製されます。
- メリットとデメリット：
    - メリット：最も厳密なデータの正確性を保証し、いつでも読み取ったデータが「ロールバック」することはありません。
    - デメリット：ネットワークの揺れ、分断、またはリーダーの障害が発生した場合、一貫性を維持するために書き込み操作がブロックされることが多く、全体的な可用性が低下します。パフォーマンスとスループットも比較的低くなります。

### 2.2 結果整合性

**結果整合性（Eventual Consistency）**は、弱い一貫性の典型的な形式であり、システムに新しい更新操作がなくなった場合、時間の経過とともに、すべてのレプリカのデータが徐々に同じ状態に収束することを要求するだけです。その間、ユーザーがレプリカデータを読み取ると、古い値が表示される可能性がありますが、最終的には一貫性が保たれます。

- **一般的な実装**：Gossipプロトコル、マルチレプリカ非同期レプリケーション、CRDT（Conflict-free Replicated Data Type）など。
- メリットとデメリット：
    - メリット：高可用性、高スループット、書き込み操作の遅延が少なく、ネットワーク分断に対する耐性が高い。
    - デメリット：短時間でのデータ不整合を許容する必要があり、アプリケーションロジックがより複雑になり、競合検出とマージが必要になる場合があります。

------

## 三、一般的な一貫性プロトコルとアルゴリズム

分散システムのレプリカ間で一貫性を保つために、業界では多くの古典的なアルゴリズムとプロトコルが提案されています。以下に、いくつかの簡単な紹介をします。

### 3.1 Paxos

Paxosは、1990年代にLeslie Lamportによって提案された分散一貫性アルゴリズムであり、主に強い一貫性または線形一貫性を実現するために使用されます。

- **基本原理**：役割分担（提案者Proposer、承認者Acceptor、学習者Learner）を通じて、複数回の投票を行い、操作または値が多数のノードによって受け入れられるかどうかを決定します。
- メリットとデメリット：
    - メリット：ネットワーク分断やノード障害が発生した場合でも、一貫性を達成でき、非常に高い安全性を備えています。
    - デメリット：実装が複雑で、デバッグとトラブルシューティングが難しく、複数回の投票によりパフォーマンスが制限されます。業界では、そのバリアント（Multi-Paxosなど）がよく使用されます。

### 3.2 Raft

Raftは2013年に正式に提案され、**Paxosと同等の安全性を保証することを前提に、実装と理解の難易度を簡素化する**ことを目標としています。安定した**リーダー（Leader）**の役割を確立し、集中型でログの複製と障害回復を行います。

- **重要な段階**：リーダー選出（Leader Election）、ログ複製（Log Replication）、安全性（Safety）など。
- **一般的な応用**：Etcd、Consul、TiKV、LogCabinなどは、Raftに基づいて強い一貫性のある複製を実現しています。
- メリットとデメリット：
    - メリット：比較的理解しやすく、実装コード量が少ない。中小規模のクラスターではパフォーマンスが良好です。
    - デメリット：主ノード（Leader）に依存しており、主ノードの障害や分断は、一時的な書き込みブロックを引き起こします。大規模なクラスターや地域をまたがるデプロイでは、遅延と可用性が影響を受ける可能性があります。

### 3.3 Gossipプロトコル

Gossip（噂）プロトコルは、従来のコンセンサスプロトコルではなく、主に分散型のシナリオで、ノードがランダムに相互作用してメタデータや状態情報を交換し、ネットワーク全体に拡散および収束させるために使用されます。

- **特徴**：分散型、低コスト、ノード間で定期的にランダムにメッセージを交換します。
- **一般的な応用**：Cassandra、Riak、分散メンバー管理（Serfなど）などで、結果整合性、レプリカ状態の同期などを実現するために使用されます。
- メリットとデメリット：
    - メリット：拡張性が高く、実装が簡単で、一貫性の要求が低く、拡張性の要求が高いシナリオに適しています。
    - デメリット：一貫性の保証が弱く、競合を最終的に解決するために、より高度な競合処理手段（CRDT、バージョン番号のマージなど）が必要です。

### 3.4 2PC / 3PC

分散トランザクションのシナリオでは、一般的なコミットプロトコルは**2PC（Two-phase Commit）**と**3PC（Three-phase Commit）**です。

- **2PC**：コーディネーターはすべての参加者に「プリコミット（prepare）」を通知し、すべて成功した場合は「コミット（commit）」をブロードキャストし、それ以外の場合は「ロールバック（abort）」します。
- **3PC**：2PCに基づいて段階を追加し、単一障害点によるブロックを減らしますが、実装がより複雑になり、極端なネットワーク分断や障害シナリオでは依然として利用できない問題があります。
- メリットとデメリット：
    - メリット：理解しやすく、トランザクションセマンティクスが明確で、分散データベースやメッセージキューなどで広く使用されています。
    - デメリット：コーディネーターへの依存性が高く、ブロックのリスクがあります。ネットワークが長期間分断された場合、トランザクションを続行できない可能性があります。

------

## 四、CAPの2つの主流な選択：CPとAP

**P**が「必須」の属性であると認識した場合、分散システムがネットワーク分断時にサービスを提供し続けるためには、**C**と**A**の間で選択する必要があります。一般的なシステム設計は、**CP**と**AP**の2つの主要な陣営に分かれます。

### 4.1 CPシステム

**CP（Consistency + Partition tolerance）**：ネットワーク分断が発生した場合、システムは**一貫性を優先的に保証**し、必要に応じて**可用性を犠牲**にします。

- 典型的な実装：
    - 多数派コンセンサス（Paxos、Raftなど）。書き込みを許可するには、過半数のノードが存続し、合意に達する必要があります。
    - 現在、クォーラム（法定数）に達することができない場合、または主ノードが故障した場合、システムは書き込み操作をブロックまたは拒否し、脳分裂によるデータ不整合を防ぎます。
- 一般的な応用：
    - Zookeeper、Etcd、Consul、分散ロックサービス、分散メタデータ管理など。
    - 金融取引のコアプロセス、銀行の会計システムなど、高い一貫性が要求されるシナリオ。
- 特徴：
    - 厳格なデータ保証：二重の主ノードやデータの混乱が発生するよりも、停止することを選択します。
    - 一定の可用性を犠牲にする：ネットワーク分断やフェイルオーバーが発生した場合、サービスが利用できないか、書き込み操作が拒否される期間があります。

### 4.2 APシステム

**AP（Availability + Partition tolerance）**：ネットワーク分断が発生した場合、システムは**可用性を優先的に保証**し、同時に**一貫性を緩和**します。

- 典型的な実装：
    - 結果整合性、マルチマスターレプリケーション、Gossipプロトコル、Dynamoスタイルの調整可能な一貫性ポリシーなど。
- 一般的な応用：
    - NoSQLデータベース（Cassandra、Riak、DynamoDBなど）、分散キャッシュシステム（Redis Cluster）など。
    - ソーシャルネットワーク、ログ収集、レコメンデーションシステムなど、高可用性、高スループットが必要で、データの一貫性に対する要求が比較的緩いビジネス。
- 特徴：
    - 分断が発生した場合でも、すべてのノードが読み取りおよび書き込みリクエストを受け入れ、システムが「可能な限り利用可能」であることを保証します。
    - データに一時的な不整合が存在する可能性がありますが、非同期同期、競合マージなどの方法でバックグラウンドで徐々に収束します。

------

## 五、CPとAPのどちらを選択するか？

実際の大規模な分散システムでは、**単一のモデルにのみ依存することはほとんどなく**、さまざまなデータまたはビジネスシナリオに対して階層化された処理を行い、**一貫性**と**可用性**の最適なバランスを追求します。

1. **コアデータにはCPを選択**
    - ユーザーアカウントの残高、注文支払い、金融取引フローなど、一貫性が非常に要求されるもの。
    - ネットワークの揺れによる一時的な書き込み不可を許容しますが、残高や取引金額のエラーは許容できません。
2. **エッジまたはキャッシュデータにはAPを選択**
    - 商品詳細ページのキャッシュ、ユーザー行動ログ、レコメンデーション候補リストなど、一貫性の要求が低いもの。
    - 高並行性、高可用性を重視し、一定時間の遅延更新やダーティリードを許容できます。

多くのインターネット企業は**ハイブリッドアーキテクチャ**を採用しています。コアトランザクションプロセスにはCPスタイルのストレージ（分散リレーショナルデータベースや強い一貫性のある分散ストレージなど）を使用し、周辺ビジネスや「読み取りが多い書き込みが少ない」シナリオにはAPスタイルのストレージまたはキャッシュソリューションを使用します。

------

## 六、CPとAPはどのように高並行性と結果整合性を実現するか

### 6.1 CPシステムはどのように高並行性に対応するか

コンセンサスプロトコルは、単一のクラスターノードの規模と書き込みリクエストの量が多い場合、高い遅延と低いスループットに直面しますが、以下の方法で並行性と拡張性を向上させることができます。

1. バッチ読み書き
    - 複数の書き込み操作をクライアントまたは中間層でパッケージ化し、リーダーノードに一度に書き込むことで、ネットワークラウンドトリップとプロトコルラウンドを削減します。
2. データベース分割とテーブル分割＆マルチクラスター
    - データを論理的またはハッシュで複数のクラスター（シャーディング）に分割します。各クラスター内では、引き続きCPプロトコルを実行します。リクエストは、ルーティングまたはプロキシ層を介して異なるシャードに分散されます。
    - 全体的な並行性を向上させ、障害の影響を単一のシャード範囲に制限します。

> CPシステムの単一シャードクラスターのスループットは、APシステムよりも2〜10倍低いことがよくあります。

### 6.2 APシステムはどのように結果整合性を保証するか

APシステムは通常、高い書き込みスループットと読み取り可用性を提供できますが、一貫性を緩和するため、バックグラウンドまたはビジネスロジック層で一貫性収束の保証を実装する必要があります。

1. バージョン番号（Vector Clock）または論理タイムスタンプ
    - 各更新操作にバージョン番号（またはLamport Clock / Hybrid Clockに基づく）を割り当て、競合シナリオでマージするか、タイムスタンプに基づく優先戦略（Last
      Write Wins）を使用します。
2. Gossipプロトコル/反エントロピー（Anti-entropy）メカニズム
    - ノードは定期的に最新のデータまたはメタデータを交換し、競合を発見した場合はマージします。
3. 調整可能な一貫性ポリシー
    - Dynamoモデルを代表として、クライアントは`R`、`W`などのパラメーター（多数派への書き込み、レプリカ確認など）を設定でき、一貫性と可用性の間で柔軟に調整できます。
4. カスタム競合解決ポリシー
    - ビジネスセマンティクスと組み合わせてマージします。たとえば、ショッピングカートは「和集合」でマージし、カウンターはCRDT（G-counter、PN-counterなど）を使用してデータの単調性を保証します。

------

## 七、CPのクロスシャードの強い一貫性の実現

第7章で述べたように、**データベース分割とテーブル分割（シャーディング）**を使用すると、単一のCPクラスターの負荷を複数のサブクラスターに「分割」して、より高い並行性をサポートできます。ただし、ビジネスがクロスシャードトランザクション（複数のデータベースまたはテーブルの更新を含む）を実行する必要がある場合、依然として**マルチシャードの一貫性**という課題に直面します。通常、次の考え方があります。

1. **分散トランザクション：2PC / 3PC**
    - アプリケーションが複数のシャードにまたがってアトミックな更新を実行する必要がある場合は、通常、分散トランザクションプロトコル（2PC、3PCなど）を使用して、各シャードのコミットまたはロールバックを調整します。
    - 問題と対策：
        - 2PC/3PCはどちらもコーディネーターノードに依存しており、単一のボトルネックになる可能性があります。
        - ネットワーク分断が深刻な場合やコーディネーターが故障した極端な状況では、ブロックが発生する可能性があります。
        - 一般的に、マスター/スレーブの切り替え、ハートビート検出とタイムアウトメカニズム、べき等再試行、MVCCなどを使用して、ブロックの影響とデータ不整合のリスクを軽減します。
2. **セルベース（Cell-based）アーキテクチャ**
    - ビジネスを複数の自律ユニットに分割します。各ユニット内のデータは同じシャードセットにあり、ほとんどのトランザクションが単一のユニット内で完了することを保証し、クロスシャード操作を減らします。
    - ユニット境界で非同期または結果整合性メカニズムを使用してデータを交換し、全体的な高可用性と一貫性を両立させます。
3. **グローバル分散データベース+グローバルコンセンサスプロトコル**
    - たとえば、Google Spannerは、各シャード（Shard）でPaxosを使用してレプリカの強い一貫性のある複製を実現し、TrueTime APIを使用してグローバルタイムスタンプを提供し、クロスシャードの一貫性を保証します。
    - このソリューションは実装が非常に複雑ですが、グローバル範囲でほぼ強い一貫性のある分散トランザクション機能を提供できます。

> **まとめ**：強い一貫性が厳密に要求されるクロスシャードトランザクションの場合、**2PC/3PC + コーディネーター**が依然として一般的なソリューションであり、コーディネーターの高可用性を可能な限り向上させることで、障害の可能性を減らします。ただし、エンジニアリングの実践では、クロスシャード書き込み操作をできるだけ減らすか、セル化の考え方を使用して、ほとんどのトランザクションを単一のシャード範囲に制限し、システムの複雑さを軽減する必要があります。

------

## 八、有名な事例の議論

以下に、業界でよく言及されるいくつかの分散システムについて簡単に説明し、それらがCAPでどのようにトレードオフを行い、実装しているかを見てみましょう。

1. **Google Spanner**
    - 典型的な**CP**システム（外部でよく言われる「CA」の錯覚を実現することもできますが、実際には依然として可用性の一部を犠牲にする必要があります）。
    - TrueTimeが提供する外部の正確なタイムスタンプと、各シャード内部のPaxos複製を利用して、データセンター間の強い一貫性を保証します。
    - グローバルな金融取引や高い一貫性が要求されるシナリオに適していますが、インフラストラクチャのコストが非常に高くなります。
2. **BigTable / HBase**
    - 表面的には**CP**に偏っており、RegionServerとMasterの間で分散調整を通じてメタデータの一貫性を保証します。
    - ただし、実際の読み取り/書き込みパスでは、マルチレプリカ非同期複製を通じて一定の高可用性手段を提供することもでき、読み取り一貫性はアプリケーションのニーズに応じて調整できます。
3. **AWS DynamoDB**
    - **AP**に傾倒しており、初期の設計はDynamo論文に触発されたもので、`R`、`W`などのパラメーターで一貫性レベルを調整できます。
    - デフォルトモードでは、非常に高い可用性と結果整合性を提供しますが、「強い一貫性のある読み取り」を有効にすることもできます（ただし、単一パーティションの強い一貫性のみが保証され、必ずしもクロスパーティションではありません）。
4. **Cassandra**
    - 同じく**AP**に傾倒しており、基盤ではGossipプロトコルを使用してノードトポロジの状態を維持します。
    - 読み取り/書き込みの一貫性は、読み取り/書き込みレプリカ数`R` / `W`を設定して、結果整合性からより強い一貫性へのスムーズな移行を実現できます。

> **比較からわかること**：エンジニアリングには絶対的な「APまたはCP」は存在せず、より多くの一貫性ポリシーの混合です。ほとんどのシステムは、さまざまなアプリケーションシナリオに適応するために、ある程度調整可能な一貫性を提供します。

------

## 九、まとめ

1. **CAP定理は一刀両断ではない**
    - 実際の分散システムでは、「Cを選んでAを放棄する」または「Aを選んでCを放棄する」と単純に言うことはできません。
    - 業界でより一般的なのは、異なるデータディメンション、異なる操作タイプに対して、柔軟に**CP**または**AP**モードを選択することです。同じシステム内でも、異なるテーブル/異なる機能に対して異なるフォールトトレランスと一貫性ポリシーを採用することもあります。
2. **APは絶対的に100％利用可能ではない**
    - たとえば、Cassandra、DynamoDBなども、極端なネットワーク分断やノードの大規模な障害が発生した場合、リクエストを満たすことができない状況が発生します。
    - APシステムは、設計上「レプリカが書き込み可能であれば、まず書き込む」という傾向があり、一貫性保証の一部を犠牲にして、比較的高い可用性とスループットを実現します。
3. **CPも可能な限り高可用性を実現できる**
    - Paxos/Raftは、通常の場合、99.99％以上の可用性を提供できますが、より多くのネットワーク、ハードウェア、エンジニアリングコストを投入する必要があり、極端なネットワーク分断が発生した場合は、書き込みをブロックし、可用性を犠牲にして一貫性を維持します。
4. **ハイブリッドアーキテクチャが主流**
    - コアトランザクションシナリオでは強い一貫性（CP）を維持し、周辺補助シナリオまたはキャッシュチャネルでは弱い一貫性（AP）を採用し、両者が相互に連携します。
    - ビジネスの許容度、ネットワーク環境、コスト投入、チームの技術的蓄えを組み合わせて、総合的にトレードオフを行う必要があります。

CAP定理は、分散システムの設計に高レベルの思考フレームワークを提供し、ネットワーク分断という避けられない現実の前で合理的な意思決定を行うのに役立ちます。実際のシステムでは、より豊富な**一貫性モデル**、**コンセンサスプロトコル**、**マルチレプリカ複製メカニズム**、およびエンジニアリングの実践（災害復旧、ダウングレード、べき等性、競合マージなど）を利用して、一貫性と可用性のバランスを取る必要があります。