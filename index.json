[{"content":" I. The CAP Theorem # 1.1 What is the CAP Theorem? # The CAP Theorem was proposed by Eric Brewer in 2000. Its core idea is:\nC (Consistency): All nodes in the system see the same data at the same time. More strictly, when a client reads data, the result should be consistent with the latest committed data, regardless of which replica is read (usually referring to strong consistency/linearizability). A (Availability): The system can still provide normal services when partial failures occur. Each request can receive a valid response within a reasonable time. P (Partition tolerance): The system can tolerate network partitions (unreachable communication between nodes). Even if the network is split, the system can provide a certain degree of availability or consistency. In a real distributed environment, network partitions are unavoidable, so P is basically regarded as a \u0026ldquo;must-have\u0026rdquo;. When network partitions occur, the system cannot simultaneously ensure the strong consistency and high availability of data across all nodes. It can only make a trade-off between C and A, resulting in two main types: CP and AP.\n1.2 Limitations of the CAP Theorem # It should be noted that the CAP Theorem itself is a relatively high-level theory, used for conceptual guidance, and should not be simply understood as \u0026ldquo;either choose C or choose A\u0026rdquo;. There are some common misunderstandings:\nC is not necessarily strong consistency The C in the CAP Theorem often refers to consistency in the strictest sense (i.e., linearizability). However, in actual systems, we have many fine-grained models to choose from, such as weak consistency, Read Committed, and Causal Consistency. Availability is not 0 or 1 It is not that choosing CP means availability is completely sacrificed; or choosing AP means consistency is not guaranteed at all. Both availability and consistency have different degrees of trade-off space and degradation strategies. Eventual consistency does not violate CAP It is a very common compromise, using lower write consistency in exchange for higher availability and throughput, and converging data in the background asynchronously. Therefore, the CAP Theorem should be combined with various consistency models and high-availability architectural patterns in specific scenarios to produce real practical guidance.\nII. Consistency Models in Distributed Systems # Consistency models are classified in many ways, but the common mainstream models can be roughly divided into: strong consistency and weak consistency (which includes eventual consistency, causal consistency, etc.). This article mainly introduces strong consistency and eventual consistency, and explains their common applications in CP or AP modes.\n2.1 Strong Consistency # Strong Consistency, also known as Linearizability, means that once a write operation is completed and returns successfully, any subsequent read operation can read the updated content. That is, the system behaves as if all operations are executed serially.\nCommon Implementation: Relies on synchronous replication and a quorum (majority) mechanism, using protocols (such as Paxos/Raft) to ensure that there is only one valid leader in the system. All operations are written to the log in order and replicated to the majority of nodes. Advantages and Disadvantages: Advantages: Guarantees the strictest data correctness. The data read at any time will not \u0026ldquo;revert\u0026rdquo;. Disadvantages: In the event of network jitter, partitions, or leader failures, write operations are often blocked to maintain consistency, leading to a decrease in overall availability; performance and throughput are also relatively lower. 2.2 Eventual Consistency # Eventual Consistency is a typical form of weak consistency. It only requires that if the system no longer has new update operations, the data of all replicas will gradually converge to the same state over time. During this period, users may see outdated values when reading replica data, but it will eventually become consistent.\nCommon Implementation: Gossip protocol, asynchronous multi-replica replication, CRDT (Conflict-free Replicated Data Type), etc. Advantages and Disadvantages: Advantages: High availability, high throughput, low write operation latency, and high tolerance for network partitions. Disadvantages: Requires tolerance for short-term data inconsistencies, more complex application logic, and may require conflict detection and merging. III. Common Consistency Protocols and Algorithms # To keep replicas in a distributed system consistent, the industry has proposed many classic algorithms and protocols. Here is a brief introduction to a few:\n3.1 Paxos # Paxos is a distributed consensus algorithm proposed by Leslie Lamport in the 1990s, mainly used to achieve strong consistency or linearizability.\nBasic Principle: Through role division (Proposer, Acceptor, Learner), multiple rounds of voting are conducted to determine whether an operation or value is accepted by the majority of nodes. Advantages and Disadvantages: Advantages: Can still reach a consensus under network partitions and node failures, with high security. Disadvantages: Complex to implement, difficult to debug and troubleshoot, and performance is limited due to multiple rounds of voting. Variants (Multi-Paxos, etc.) are more commonly used in the industry. 3.2 Raft # Raft was officially proposed in 2013, with the goal of simplifying implementation and understanding while ensuring the same level of security as Paxos. It establishes a stable Leader role to centrally perform log replication and fault recovery:\nKey Stages: Leader Election, Log Replication, Safety, etc. Common Applications: Etcd, Consul, TiKV, LogCabin, etc., are all based on Raft to achieve strong consistent replication. Advantages and Disadvantages: Advantages: Relatively easy to understand, less implementation code; good performance for small and medium-sized clusters. Disadvantages: Relies on the master node (Leader), and leader failures or partitions can cause temporary write blocking; in large-scale clusters or cross-regional deployments, latency and availability will be affected. 3.3 Gossip Protocol # The Gossip protocol is not a traditional consensus protocol. It is mainly used in decentralized scenarios to exchange metadata or status information through random interactions between nodes, thereby spreading and converging information across the entire network.\nFeatures: Decentralized, low overhead, and nodes exchange messages periodically and randomly. Common Applications: Cassandra, Riak, distributed membership management (such as Serf), etc., are used to achieve eventual consistency, replica state synchronization, etc. Advantages and Disadvantages: Advantages: Good scalability, simple to implement, suitable for scenarios with low consistency requirements and high scalability requirements. Disadvantages: Weak consistency guarantees, requires higher-level conflict handling methods (such as CRDT, version number merging, etc.) to ultimately resolve conflicts. 3.4 2PC / 3PC # In distributed transaction scenarios, common commit protocols are 2PC (Two-phase Commit) and 3PC (Three-phase Commit):\n2PC: The coordinator notifies all participants to \u0026ldquo;prepare,\u0026rdquo; and if all are successful, it broadcasts \u0026ldquo;commit,\u0026rdquo; otherwise \u0026ldquo;abort.\u0026rdquo; 3PC: Adds a stage on top of 2PC to reduce blocking caused by single point failures, but is more complex to implement and still has unavailability issues in extreme network partition or failure scenarios. Advantages and Disadvantages: Advantages: Easy to understand, clear transaction semantics, widely used in distributed databases, message queues, etc. Disadvantages: Strong dependence on the coordinator, risk of blocking; may not be able to continue to advance transactions when the network is partitioned for a long time. IV. Two Main Choices of CAP: CP and AP # After we determine that P is a \u0026ldquo;must-have\u0026rdquo; attribute, if a distributed system wants to continue providing services during network partitions, it must make a choice between C and A. Common system designs are therefore divided into two major camps: CP and AP.\n4.1 CP System # CP (Consistency + Partition tolerance): When encountering network partitions, the system will choose to prioritize consistency and sacrifice availability when necessary.\nTypical Implementation: Majority consensus (Paxos, Raft, etc.), requiring more than half of the nodes to be alive and reach an agreement before allowing writes. If a quorum cannot be reached or the master node fails, the system will block or reject write operations to prevent data inconsistencies caused by split-brain. Common Applications: Zookeeper, Etcd, Consul, distributed lock services, distributed metadata management, etc. Core financial transaction processes, bank accounting systems, and other scenarios with high consistency requirements. Characteristics: Strict data guarantees: Rather than having dual masters or data chaos, it would rather shut down. Sacrifices some availability: During network partitions or failovers, there will be a window where the service is unavailable or rejects write operations. 4.2 AP System # AP (Availability + Partition tolerance): When encountering network partitions, the system will choose to prioritize availability and relax consistency.\nTypical Implementation: Eventual consistency, multi-master replication, Gossip protocol, Dynamo-style adjustable consistency policies, etc. Common Applications: NoSQL databases (Cassandra, Riak, DynamoDB, etc.), distributed caching systems (Redis Cluster), etc. Social networks, log collection, recommendation systems, and other businesses that require high availability and high throughput, and have relatively relaxed requirements for data consistency. Characteristics: Even during partitions, all nodes still accept read and write requests, ensuring the system is \u0026ldquo;as available as possible.\u0026rdquo; Data may be temporarily inconsistent, but will gradually converge in the background through asynchronous synchronization and conflict merging. V. How to Choose Between CP and AP? # In real large-scale distributed systems, it is often rare to rely on a single model. Instead, different data or business scenarios are processed in layers to achieve the optimal balance between consistency and availability.\nChoose CP for core data Such as user account balances, order payments, financial transaction flows, etc., which have extremely high consistency requirements. Tolerates temporary unwriteability caused by network jitter, but cannot tolerate errors in balances or transaction amounts. Choose AP for edge or cached data Such as cached product detail pages, user behavior logs, recommendation candidate lists, etc., which have lower consistency requirements. More emphasis is placed on high concurrency and high availability, and it can tolerate a certain amount of delayed updates or dirty reads. Many internet companies use hybrid architectures: the core transaction processes use CP-style storage (such as distributed relational databases or distributed storage with strong consistency); peripheral businesses or \u0026ldquo;read-heavy\u0026rdquo; scenarios use AP-style storage or caching solutions.\nVI. How CP and AP Achieve High Concurrency and Eventual Consistency # 6.1 How CP Systems Cope with High Concurrency # Although consensus protocols face higher latency and lower throughput when the single cluster node scale and write request volume are large, concurrency and scalability can still be improved through the following methods:\nBatch Read and Write Pack multiple write operations on the client or middle layer and write them to the leader node at once, reducing network round trips and protocol rounds. Database Sharding \u0026amp; Multiple Clusters Split data into multiple clusters (sharding) by logic or hash. Each cluster still runs the CP protocol internally; requests are distributed to different shards through routing or proxy layers. Improves overall concurrency and limits the impact of failures to a single shard. The single-shard cluster throughput of CP systems is often 2 to 10 times lower than that of AP systems.\n6.2 How AP Systems Ensure Eventual Consistency # AP systems can usually provide high write throughput and read availability, but they relax consistency. Therefore, it is necessary to implement consistency convergence guarantees in the background or business logic layer:\nVersion Number (Vector Clock) or Logical Timestamp Assign a version number to each update operation (or based on Lamport Clock / Hybrid Clock), and merge in conflict scenarios or use a timestamp-based win strategy (Last Write Wins). Gossip Protocol / Anti-entropy Mechanism Nodes periodically exchange the latest data or metadata and merge if conflicts are discovered. Tunable Consistency Policies Represented by the Dynamo model, clients can configure parameters such as R and W (such as majority writes, replica confirmation), thereby flexibly adjusting between consistency and availability. Custom Conflict Resolution Strategies Merge based on business semantics, such as merging shopping carts using \u0026ldquo;union,\u0026rdquo; and using CRDT (G-counter, PN-counter, etc.) for counters to ensure data monotonicity. VII. Implementation of Cross-Shard Strong Consistency in CP Systems # As mentioned in Chapter VII, database sharding can \u0026ldquo;split\u0026rdquo; the pressure of a single CP cluster into multiple sub-clusters to support higher concurrency. However, when a business needs to perform transactions across shards (i.e., involves updates to multiple databases or tables), it still faces the challenge of multi-shard consistency. There are usually the following approaches:\nDistributed Transactions: 2PC / 3PC If the application needs to perform atomic updates across multiple shards, distributed transaction protocols (such as 2PC, 3PC) are usually used to coordinate the commit or rollback of each shard. Problems and Solutions: 2PC/3PC both rely on a coordinator node, which can become a single point of bottleneck. In extreme cases of severe network partitions or coordinator failures, blocking may occur. Generally, master-slave switching, heartbeat detection and timeout mechanisms, idempotent retries, MVCC, etc., are used to reduce the impact of blocking and the risk of data inconsistency. Cell-based Architecture Divide the business into multiple autonomous units. The data in each unit is in the same shard set, ensuring that most transactions are completed in a single unit, reducing cross-shard operations. Asynchronous or eventual consistency mechanisms are used at unit boundaries for data exchange, taking into account the overall high availability and consistency. Global Distributed Database + Global Consensus Protocol For example, Google Spanner uses Paxos to achieve strong consistent replication of replicas on each shard, and then uses the TrueTime API to provide global timestamps to ensure cross-shard consistency. This solution is extremely complex to implement, but it can provide near-strong consistent distributed transaction capabilities globally. Summary: For cross-shard transactions that strictly require strong consistency, 2PC/3PC + Coordinator is still a common solution. The possibility of failures is reduced by maximizing the high availability of the coordinator. However, in engineering practice, cross-shard write operations should be minimized as much as possible, or the complexity of the system should be reduced by using a cell-based approach to limit most transactions to a single shard.\nVIII. Discussion of Famous Cases # Here is a brief discussion of several distributed systems that are often mentioned in the industry, and their trade-offs and implementations in CAP:\nGoogle Spanner A typical CP system (it can even achieve the \u0026ldquo;CA\u0026rdquo; illusion often mentioned externally, but in essence, it still needs to sacrifice some availability). It uses the external precise timestamps provided by TrueTime + Paxos replication within each shard to ensure strong consistency across data centers. Suitable for global financial transactions or scenarios with high consistency requirements, but the infrastructure cost is extremely high. BigTable / HBase On the surface, it is more inclined to CP. Distributed coordination is used between RegionServer and Master to ensure the consistency of metadata. However, in the actual read and write paths, it can also provide certain high-availability measures through asynchronous multi-replica replication, and read consistency can be adjusted according to application requirements. AWS DynamoDB Inclined to AP. The early design was inspired by the Dynamo paper, and consistency levels can be adjusted through parameters such as R and W. In the default mode, it provides extremely high availability and eventual consistency, and \u0026ldquo;strong consistent reads\u0026rdquo; can also be enabled (but only guarantees strong consistency within a single partition, not necessarily across partitions). Cassandra Also AP inclined, the underlying layer uses the Gossip protocol to maintain the node topology status. Read and write consistency can be configured by the number of read and write replicas R / W to achieve a smooth transition from eventual consistency to stronger consistency. Comparison shows: In engineering, there is no absolute \u0026ldquo;AP or CP\u0026rdquo;. It is more of a mixture of multiple consistency strategies. Most systems provide a certain degree of adjustable consistency to adapt to different application scenarios.\nIX. Conclusion # The CAP Theorem is not a one-size-fits-all solution Real distributed systems cannot simply say \u0026ldquo;I choose C and give up A\u0026rdquo; or \u0026ldquo;I choose A and give up C.\u0026rdquo; What is more common in the industry is to flexibly choose CP or AP mode for different data dimensions and different operation types. Even within the same system, different fault tolerance and consistency strategies are adopted for different tables/functions. AP is not absolutely 100% available For example, Cassandra, DynamoDB, etc., may also fail to meet requests in extreme network partitions or when a large number of nodes fail. AP systems are designed to \u0026ldquo;write as long as the replica is writable,\u0026rdquo; sacrificing some consistency guarantees in exchange for relatively higher availability and throughput. CP can also try to achieve high availability Paxos/Raft can also provide 99.99% or even higher availability under normal circumstances, but it requires more investment in network, hardware, and engineering costs. In extreme network partitions, it will still block writes and sacrifice availability to maintain consistency. Hybrid architecture is the mainstream Core transaction scenarios adhere to strong consistency (CP), and peripheral auxiliary scenarios or caching channels use weak consistency (AP), and the two cooperate with each other. It is necessary to make a comprehensive trade-off based on business tolerance, network environment, cost investment, and team technical reserves. The CAP Theorem provides a high-level framework for thinking about the design of distributed systems, helping us make rational decisions in the face of the unavoidable reality of network partitions. In actual systems, it is necessary to use richer consistency models, consensus protocols, multi-replica replication mechanisms, and engineering practices (disaster recovery, degradation, idempotency, conflict merging, etc.) to balance consistency and availability.\n","date":"20 December 2024","externalUrl":null,"permalink":"/posts/1735350760948-cap/","section":"Posts","summary":"Discussing the Application of the CAP Theorem in Distributed Systems from Theory to Practice.","title":"In-depth explanation of the CAP theorem: Building high-concurrency and high-availability distributed systems","type":"posts"},{"content":"","date":"26 December 2024","externalUrl":null,"permalink":"/tags/algorithm/","section":"Tags","summary":"","title":"Algorithm","type":"tags"},{"content":"","date":"26 December 2024","externalUrl":null,"permalink":"/","section":"David Zhang","summary":"","title":"David Zhang","type":"page"},{"content":"","date":"26 December 2024","externalUrl":null,"permalink":"/tags/divide-and-conquer-algorithm/","section":"Tags","summary":"","title":"Divide and Conquer Algorithm","type":"tags"},{"content":"","date":"26 December 2024","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"26 December 2024","externalUrl":null,"permalink":"/tags/quick-sort/","section":"Tags","summary":"","title":"Quick Sort","type":"tags"},{"content":"Quick sort is a comparison-based unstable sorting algorithm that employs the divide-and-conquer strategy. It has an average time complexity of $O(n\\log n)$ and a worst-case time complexity of $O(n^2)$, with a space complexity of $O(1)$. The following will use sorting an integer sequence in ascending order as an example to introduce its implementation details and common mistakes.\nProblem Description # Given an integer sequence of length $n$, sort it in ascending order using quick sort and output the result.\nInput Format # The first line contains an integer $n$. The second line contains $n$ integers, all within the range $[1, 10^9]$. Output Format # Output the sorted sequence on a single line. Data Range # $1 \\leq n \\leq 100000$\nInput Example # 5 3 1 2 4 5 Output Example # 1 2 3 4 5 Quick Sort Idea # During each divide step in quick sort, a number is arbitrarily chosen as the pivot (the number in the middle position is chosen below).\nUse left and right pointers moving towards each other. The left pointer L searches from left to right for the first number greater than or equal to the pivot, and the right pointer R searches from right to left for the first number less than or equal to the pivot. Then, swap these two numbers.\nRepeat this process until the left and right pointers overlap or the left pointer is one position to the right of the right pointer. This is called one cycle.\nAfter each pointer movement and swap, ensure that the structure \u0026ldquo;left part ≤ pivot, right part ≥ pivot\u0026rdquo; is not broken, i.e., there is an invariant [left, L) \u0026lt;= pivot, (R, right] \u0026gt;= pivot.\nIn the example code below, left and right are the boundaries of the currently processed closed interval, and pivot is taken as the element at the midpoint of the interval.\n1#include \u0026lt;bits/stdc++.h\u0026gt; 2using namespace std; 3 4void quickSort(vector\u0026lt;int\u0026gt; \u0026amp;a, int left, int right) { 5 if (left \u0026gt;= right) return; 6 7 int pivot = a[(left + right) / 2]; 8 int l = left, r = right; 9 10 while (true) { 11 while (a[l] \u0026lt; pivot) l++; 12 while (a[r] \u0026gt; pivot) r--; 13 if (l \u0026gt;= r) break; 14 swap(a[l], a[r]); 15 l++; r--; 16 } 17 18 quickSort(a, left, r); 19 quickSort(a, r + 1, right); 20} 21 22int main() { 23 int n; cin \u0026gt;\u0026gt; n; 24 vector\u0026lt;int\u0026gt; a(n); 25 for (int i = 0; i \u0026lt; n; i++) cin \u0026gt;\u0026gt; a[i]; 26 27 quickSort(a, 0, n - 1); 28 29 for (int i = 0; i \u0026lt; n; i++) cout \u0026lt;\u0026lt; a[i] \u0026lt;\u0026lt; \u0026#34; \u0026#34;; 30 return 0; 31} Complexity and pivot Selection # Since quick sort has a worst-case complexity of $O(n^2)$, the selection of the pivot is crucial. If the first or last element is always chosen, the worst case is likely to occur in nearly sorted arrays.\nIn addition to taking the element in the middle position, a random element can also be selected as the pivot, or the median of the left, middle, and right elements can be taken as the pivot.\nCommon Error Examples # The following code contains several common errors.\n1#include \u0026lt;bits/stdc++.h\u0026gt; 2using namespace std; 3 4void quickSort(vector\u0026lt;int\u0026gt; \u0026amp;a, int left, int right) { 5 if (left == right) return; // 7 6 7 int pivot = (left + right) \u0026gt;\u0026gt; 1; // 1 8 int l = left, r = right; 9 10 while (true) { 11 while (a[l] \u0026lt;= pivot) l++; // 2 12 while (a[r] \u0026gt;= pivot) r--; // 2 13 swap(a[l], a[r]); 14 if (l \u0026gt;= r) break; // 3 15 // 4 16 } 17 18 quickSort(a, left, l - 1); // 5, 6 19 quickSort(a, l, right); // 5, 6 20} 21 22int main() { 23 int n; cin \u0026gt;\u0026gt; n; 24 vector\u0026lt;int\u0026gt; a(n); 25 for (int i = 0; i \u0026lt; n; i++) cin \u0026gt;\u0026gt; a[i]; 26 27 quickSort(a, 0, n - 1); 28 29 for (int i = 0; i \u0026lt; n; i++) cout \u0026lt;\u0026lt; a[i] \u0026lt;\u0026lt; \u0026#34; \u0026#34;; 30 31 return 0; 32} Error Analysis:\npivot should be a number in the array, not an index. Use \u0026lt; and \u0026gt; instead of \u0026lt;= and \u0026gt;=, otherwise the left pointer may move more than one position past the right pointer, and the array cannot be divided into two parts. After finding l \u0026gt;= r, the loop should be exited immediately, and no more swaps should be performed. Otherwise, it cannot be guaranteed that the elements on the left are no greater than pivot and the elements on the right are no less than pivot. After each swap, l++ and r-- should be executed. The pivot is actually taking the number in the middle-left position. Therefore, if using $l - 1$ and $l$ to divide the array, consider the array [1, 2]. It is easy to see that it will lead to an infinite loop, continuously dividing the array into two parts of size 0 and 2. Similarly, using $r$ and $l$ to divide the array will not work either. Instead, at the end of a cycle, $r$ must be less than $right$, so $r$ and $r+1$ can be used to divide the array. Readers can simulate the algorithm process to see why. Another simple way to avoid infinite loops is to randomly select the pivot or handle the case where there are only two elements specially. Also, using $l$, $l+1$ is not correct either, because this division does not conform to the definition. When $r$ is to the left of $l$, using $l$, $l+1$ cannot correctly divide the array into two parts where the left part is less than or equal to pivot and the right part is greater than or equal to pivot. This problem assumes that the array is not empty, so the case of \u0026gt; does not exist. However, it is recommended to use \u0026gt;=, which is safer. Supplement # Quick sort can also be evolved into \u0026ldquo;quick select\u0026rdquo;, which can find the $k$-th smallest number in an unordered array in $O(n)$ expected time. The specific idea is similar to quick sort, except that it only continues recursion in one sub-interval each time, thus reducing the time complexity.\n","date":"26 December 2024","externalUrl":null,"permalink":"/posts/1735252761946-quick-sort/","section":"Posts","summary":"Key Points for Correctly Implementing the Quick Sort Algorithm.","title":"Quick Sort","type":"posts"},{"content":"","date":"26 December 2024","externalUrl":null,"permalink":"/tags/sorting-algorithm/","section":"Tags","summary":"","title":"Sorting Algorithm","type":"tags"},{"content":"","date":"26 December 2024","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"2024年12月26日","externalUrl":null,"permalink":"/zh-cn/tags/%E5%88%86%E6%B2%BB%E7%AE%97%E6%B3%95/","section":"Tags","summary":"","title":"分治算法","type":"tags"},{"content":"","date":"2024年12月26日","externalUrl":null,"permalink":"/zh-cn/tags/%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F/","section":"Tags","summary":"","title":"快速排序","type":"tags"},{"content":"","date":"2024年12月26日","externalUrl":null,"permalink":"/zh-cn/tags/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/","section":"Tags","summary":"","title":"排序算法","type":"tags"},{"content":"","date":"2024年12月26日","externalUrl":null,"permalink":"/zh-cn/tags/%E7%AE%97%E6%B3%95/","section":"Tags","summary":"","title":"算法","type":"tags"},{"content":" Problem # There are $N$ items. The volume of the $i$-th item is $s_i$, and its value is $v_i$. Each item can be taken only once. Under the premise of not exceeding the maximum total volume limit $S$, find the maximum total value $V$ that can be obtained.\nInput Format # The first line contains two integers, $N$ and $S$, separated by a space, representing the number of items and the maximum total volume limit, respectively. The following $N$ lines each contain two integers, $s_i$ and $v_i$, separated by a space, representing the volume and value of the $i$-th item, respectively.\nOutput Format # Output an integer representing the maximum value.\nData Range # $$0 \\le N, S \\leq 1000$$$$0 \\le s_i, v_i \\leq 1000$$ Input Example # 4 5 1 2 2 4 3 4 4 5 Output Example # 8 Solution # Define the state: f[i][j] represents the maximum value that can be obtained from the first $i$ items with a volume limit of $j$. If the $i$-th item is not taken, then f[i][j] = f[i - 1][j] If the $i$-th item is taken, then f[i][j] = f[i - 1][j - s[i]] + v[i] When implementing the state transition, pay attention to the domain range. If $j \u003c s_i$, then do not consider taking the $i$-th item. Because if $j - s_i$ is negative, the array index is invalid. It can also be explained as: the volume of the $i$-th item is greater than the volume limit, so it is impossible. Define the initial condition: For the first 0 items, any volume limit yields a value of 0, i.e., f[0][j] = 0, j $\\in [0, S]$. Time complexity: $O(NS)$. Code # 1#include\u0026lt;bits/stdc++.h\u0026gt; 2using namespace std; 3int main() { 4 int N, S; 5 cin \u0026gt;\u0026gt; N \u0026gt;\u0026gt; S; 6 vector\u0026lt;int\u0026gt; s(N + 1), v(N + 1); 7 for (int i = 1; i \u0026lt;= N; i++) cin \u0026gt;\u0026gt; s[i] \u0026gt;\u0026gt; v[i]; 8 vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; f(N + 1, vector\u0026lt;int\u0026gt;(S + 1)); 9 for (int i = 1; i \u0026lt;= N; i++) { 10 for (int j = 0; j \u0026lt;= S; j++) { 11 f[i][j] = f[i - 1][j]; 12 if (j \u0026gt;= s[i]) f[i][j] = max(f[i][j], f[i - 1][j - s[i]] + v[i]); 13 } 14 } 15 cout \u0026lt;\u0026lt; f[N][S] \u0026lt;\u0026lt; endl; 16 return 0; 17} 1D DP Optimization # Compressing the two-dimensional array into a one-dimensional array can significantly save space and improve the running speed to a certain extent (the disadvantage is that it cannot meet the special requirements of some problem types). Note that in the state transition, f[i][j] is only related to f[i - 1][j] and f[i - 1][j - s[i]]. In other words, in the two-dimensional array f in the code, f[i][j] is only related to the elements in the previous row that are to its left or in the same column. Therefore, the two-dimensional array can be compressed into a one-dimensional array or a rolling array. Note that in the code below, the second loop iterates in reverse order. This is because we need to ensure that when calculating f[i][j], f[i - 1][j - s[i]] has not been updated yet. 1#include\u0026lt;bits/stdc++.h\u0026gt; 2using namespace std; 3int main() { 4 int N, S; 5 cin \u0026gt;\u0026gt; N \u0026gt;\u0026gt; S; 6 vector\u0026lt;int\u0026gt; s(N + 1), v(N + 1); 7 for (int i = 1; i \u0026lt;= N; i++) cin \u0026gt;\u0026gt; s[i] \u0026gt;\u0026gt; v[i]; 8 vector\u0026lt;int\u0026gt; f(S + 1); 9 for (int i = 1; i \u0026lt;= N; i++) { 10 for (int j = S; j \u0026gt;= s[i]; j--) { 11 f[j] = max(f[j], f[j - s[i]] + v[i]); 12 } 13 } 14 cout \u0026lt;\u0026lt; f[S] \u0026lt;\u0026lt; endl; 15 return 0; 16} If the Number of Schemes is Required # Not only should the maximum total value that can be obtained be output, but also \u0026ldquo;how many different selection methods can achieve this maximum total value\u0026rdquo;. The following introduces how to count the number of schemes in the 01 knapsack problem.\n2D DP for Counting Schemes # The following uses 2D DP as an example for explanation.\nDefine the state:\ndp[i][j] represents \u0026ldquo;the maximum value that can be obtained when considering the first i items with a capacity ( volume limit) of j\u0026rdquo;. ways[i][j] represents \u0026ldquo;the number of schemes corresponding to the maximum value obtained when considering the first i items with a capacity of j\u0026rdquo;. State transition:\nIf the $i$-th item is not selected: $$ \\text{dp}[i][j] = \\text{dp}[i-1][j], \\quad \\text{ways}[i][j] = \\text{ways}[i-1][j] $$ If the $i$-th item is selected (provided that $ j \\ge s_i $): $$ \\text{dp}[i][j] = \\text{dp}[i-1][j - s_i] + v_i, \\quad \\text{ways}[i][j] = \\text{ways}[i-1][j - s_i] $$ Whether to select or not, the final dp[i][j] should take the larger of the two: If $$ \\text{dp}[i-1][j - s_i] + v_i $$ then it means that \"selecting the i-th item\" has a greater value: $$ \\text{dp}[i][j] = \\text{dp}[i-1][j - s_i] + v_i, \\quad \\text{ways}[i][j] = \\text{ways}[i-1][j - s_i]. $$\nIf $$ \\text{dp}[i-1][j - s_i] + v_i = \\text{dp}[i-1][j], $$ it means that the maximum value obtained by the two methods is the same, then the number of schemes should be added: $$ \\text{dp}[i][j] = \\text{dp}[i-1][j], \\quad \\text{ways}[i][j] = \\text{ways}[i-1][j] \\text{ways}[i-1][j - s_i]. $$ If $$ \\text{dp}[i-1][j - s_i] + v_i \u003c \\text{dp}[i-1][j], $$ then it means that \u0026ldquo;not selecting the i-th item\u0026rdquo; has a greater value, and the number of schemes inherits the number of schemes when not selecting: $$ \\text{dp}[i][j] = \\text{dp}[i-1][j], \\quad \\text{ways}[i][j] = \\text{ways}[i-1][j]. $$ Initial conditions:\ndp[0][j] = 0 means that when there are 0 items, the maximum value obtained for any capacity is 0. ways[0][0] = 1 means that \u0026ldquo;0 items, capacity 0\u0026rdquo; is a feasible scheme (i.e., selecting nothing), and the number of schemes is set to 1. For j \u0026gt; 0, when there are no items to choose from and the capacity is greater than 0, it is impossible to obtain any positive value, and the corresponding number of schemes is 0, i.e., ways[0][j] = 0. Final answer:\ndp[N][S] is the maximum value. ways[N][S] is the number of schemes to achieve this maximum value. Time complexity: $O(NS)$. This problem can also be optimized using 1D DP. If the Requirement is to Exactly Reach the Volume Limit # Define the state: f[i][j] represents the maximum value when the first i items have exactly a volume of $j$. If the $i$-th item is not taken, then f[i][j] = f[i - 1][j] If the $i$-th item is taken, then f[i][j] = f[i - 1][j - s[i]] + v[i] It can be noted that there is no difference in the state transition from the original problem. However, the initial conditions are different. Besides f[0][0] = 0, the rest f[0][j] = $-\\infty$, j $\\in [1, S]$. $-\\infty$ represents an impossible state. If the Volume Limit $S$ is Very Large (1e9), While the Number of Items $N$ and the Maximum Total Value $V$ are Relatively Small # For such problems, there is a solution with a complexity of $O(NV)$. Define the state: f[i][j] represents the minimum volume when selecting several items from the first i items, with a total value of exactly j. If the $i$-th item is not taken, then f[i][j] = f[i - 1][j] If the $i$-th item is taken, then f[i][j] = f[i - 1][j - v[i]] + s[i] Take the smaller of the two. Initial conditions: f[0][0] = 0, the rest f[0][j] = $\\infty$, j $\\in [1, V]$. $\\infty$ represents an impossible state. Note that it is not $-\\infty$. The final answer is the largest j in f[N][j] such that f[N][j] \u0026lt;= S. If the Volume Limit $S$ and the Value of a Single Item $v_i$ are Both Very Large (on the order of 1e9), While the Number of Items $N$ is Very Small (no more than 40 at most) # When $N \\leq 20$, all subsets can be directly enumerated by brute force (time complexity $O(2^N)$). When $N \\leq 40$, since $2^{40}$ is on the order of $10^{12}$, direct brute force will also be relatively large, so * meet-in-the-middle search* can be used to reduce the complexity to approximately $O\\bigl(2^{\\frac{N}{2}} \\times \\log(2^{\\frac{N}{2}})\\bigr) \\approx O(N \\cdot 2^{\\frac{N}{2}})$, which can be completed in an acceptable time. ","date":"24 December 2024","externalUrl":null,"permalink":"/posts/1735112650761-0-1-knapsack-problem/","section":"Posts","summary":"The most basic classic knapsack problem.","title":"0-1 knapsack problem","type":"posts"},{"content":"","date":"24 December 2024","externalUrl":null,"permalink":"/tags/binary-search/","section":"Tags","summary":"","title":"Binary Search","type":"tags"},{"content":"If an ordered solution space is divided into left and right parts, where one part satisfies a condition and the other does not, then binary search can be used to find the critical point in the ordered solution space.\nThe basic idea of binary search is to repeatedly halve the search interval. Each time, the middle element is checked. If the middle element does not satisfy the condition, half of the interval can be excluded; otherwise, the search continues in the other half. Since half of the search interval is discarded each time, the search time complexity can reach $O(\\log n)$.\nExample Problem # Problem Description: Given an ascendingly sorted integer array of length $n$, and $q$ queries. Each query gives an integer $k$, and we need to find the \u0026ldquo;starting position\u0026rdquo; and \u0026ldquo;ending position\u0026rdquo; of $k$ in the array (indices start from 0). If the number does not exist in the array, return -1 -1.\nInput Format # First line: two integers $n$ and $q$, representing the length of the array and the number of queries, respectively. Second line: $n$ integers, representing the complete array, already sorted in ascending order. Next $q$ lines: each line contains an integer $k$, representing a query element. Data Range # $1 \\leq n \\leq 100000$\n$1 \\leq q \\leq 10000$\n$1 \\leq k \\leq 10000$\nOutput Format # For each query, output the starting and ending positions of the element in the array on one line. If the element does not exist in the array, output -1 -1.\nExample:\nInput: 6 3 1 2 2 3 3 4 3 4 5 Output: 3 4 5 5 -1 -1 Explanation:\nThe range where element $3$ appears is $[3, 4]$; Element $4$ appears only once, at position $5$; Element $5$ does not exist in the array, so return $-1$ $-1$. Solution # Finding the \u0026ldquo;Starting Position\u0026rdquo;: That is, finding the first position greater than or equal to $k$. The array can be divided into two parts:\nAll numbers on the left are \u0026ldquo;less than\u0026rdquo; $k$ All numbers on the right are \u0026ldquo;greater than or equal to\u0026rdquo; $k$ The answer is the first position on the right Finding the \u0026ldquo;Ending Position\u0026rdquo;: That is, finding the last position less than or equal to $k$. The array can be divided into two parts:\nAll numbers on the left are \u0026ldquo;less than or equal to\u0026rdquo; $k$ All numbers on the right are \u0026ldquo;greater than\u0026rdquo; $k$ The answer is the last position on the left Recommended Template # Below is an elegant and less error-prone binary search template.\nDefine two pointers $l, r$, with the invariant: the closed interval $[0, l]$ all belongs to the left part, and the closed interval $[r, n - 1]$ all belongs to the right part. $l$ and $r$ are initialized to $-1$ and $n$, respectively.\nWhen the algorithm terminates, $l$ and $r$ are adjacent, pointing to the last element of the left part and the first element of the right part, respectively.\nBecause the solution we want may not exist, if the problem does not state that a solution definitely exists, we need to check if l or r is out of bounds and if it points to the correct value.\n1#include \u0026lt;bits/stdc++.h\u0026gt; 2using namespace std; 3 4int main() { 5 int n, q; 6 cin \u0026gt;\u0026gt; n \u0026gt;\u0026gt; q; 7 vector\u0026lt;int\u0026gt; nums(n); 8 for(int i = 0; i \u0026lt; n; i++) cin \u0026gt;\u0026gt; nums[i]; 9 10 while(q--) { 11 int k; 12 cin \u0026gt;\u0026gt; k; 13 14 // 1. Find the starting position of k 15 // Divide the array into two parts, the left part is all \u0026lt; k, and the right part is all \u0026gt;= k. 16 // The answer is the smallest index of the right part. 17 int l = -1, r = n; 18 while(l \u0026lt; r - 1) { 19 int mid = (l + r) / 2; 20 if(nums[mid] \u0026gt;= k) r = mid; 21 else l = mid; 22 } 23 24 // If r is out of bounds or nums[r] != k, it means k does not exist 25 if (r == n || nums[r] != k) { 26 cout \u0026lt;\u0026lt; -1 \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; -1 \u0026lt;\u0026lt; endl; 27 continue; 28 } 29 30 int leftPos = r; 31 32 // 2. Find the ending position of k 33 // Divide the array into two parts, the left part is all \u0026lt;= k, and the right part is all \u0026gt; k. 34 // The answer is the largest index of the left part. 35 l = -1, r = n; 36 while(l \u0026lt; r - 1) { 37 int mid = (l + r) / 2; 38 if(nums[mid] \u0026lt;= k) l = mid; 39 else r = mid; 40 } 41 42 int rightPos = l; 43 cout \u0026lt;\u0026lt; leftPos \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; rightPos \u0026lt;\u0026lt; endl; 44 } 45 46 return 0; 47} Advantages # This approach has strictly defined invariants. It applies to both finding the \u0026ldquo;starting position\u0026rdquo; and the \u0026ldquo;ending position\u0026rdquo; without extra handling or changes. Some approaches use l == r as the termination condition. When $l$ and $r$ differ by $1$, it will calculate $mid$ equal to $l$ or $r$. If not handled correctly, updating $l$ or $r$ to $mid$ will not shrink the search interval, leading to an infinite loop. In contrast, this approach terminates when $l$ and $r$ are adjacent, ensuring that $mid$ is less than $l$ and greater than $r$, and updating $l$ or $r$ will always shrink the search interval. STL # If you use the lower_bound and upper_bound functions provided by C++ STL, you can also accomplish the same thing:\nlower_bound(first, last, val) will return \u0026ldquo;the first position greater than or equal to val\u0026rdquo; upper_bound(first, last, val) will return \u0026ldquo;the first position greater than val\u0026rdquo; For example, suppose nums = {1,2,3,4,4,4,4,4,5,5,6}, and we want to know the interval where 4 appears:\n1vector\u0026lt;int\u0026gt; nums = {1,2,3,4,4,4,4,4,5,5,6}; 2auto it1 = lower_bound(nums.begin(), nums.end(), 4); 3auto it2 = upper_bound(nums.begin(), nums.end(), 4); 4 5if (it1 == nums.end() || *it1 != 4) { 6 cout \u0026lt;\u0026lt; \u0026#34;4 appears 0 times\u0026#34; \u0026lt;\u0026lt; endl; 7} else { 8 cout \u0026lt;\u0026lt; \u0026#34;first 4 is at \u0026#34; \u0026lt;\u0026lt; it1 - nums.begin() \u0026lt;\u0026lt; endl; 9 cout \u0026lt;\u0026lt; \u0026#34;last 4 is at \u0026#34; \u0026lt;\u0026lt; it2 - nums.begin() - 1 \u0026lt;\u0026lt; endl; 10 cout \u0026lt;\u0026lt; \u0026#34;4 appears \u0026#34; \u0026lt;\u0026lt; it2 - it1 \u0026lt;\u0026lt; \u0026#34; times\u0026#34; \u0026lt;\u0026lt; endl; 11} it1 points to the first position where the value is greater than or equal to $4$. it2 points to the first position where the value is greater than $4$. Therefore, it2 - it1 is the number of times $4$ appears in the array; it2 - nums.begin() - 1 is the position of the right boundary of $4$. Supplement # Binary search can also be extended to search in floating-point ranges (such as finding the roots of an equation), and ternary search to find the extrema of unimodal functions.\nPractice # LeetCode 33. Search in Rotated Sorted Array\nHint: First, use binary search to find the rotation point, and then use binary search to find the target value.\n","date":"24 December 2024","externalUrl":null,"permalink":"/posts/1735078223417-binary-search-intro/","section":"Posts","summary":"How to implement the binary search algorithm elegantly.","title":"Binary Search","type":"posts"},{"content":"","date":"24 December 2024","externalUrl":null,"permalink":"/series/classic-knapsack-problem-set/","section":"Series","summary":"","title":"Classic Knapsack Problem Set","type":"series"},{"content":"","date":"24 December 2024","externalUrl":null,"permalink":"/tags/dynamic-programming/","section":"Tags","summary":"","title":"Dynamic Programming","type":"tags"},{"content":"","date":"24 December 2024","externalUrl":null,"permalink":"/tags/knapsack-problem/","section":"Tags","summary":"","title":"Knapsack Problem","type":"tags"},{"content":"","date":"24 December 2024","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","date":"2024年12月24日","externalUrl":null,"permalink":"/zh-cn/tags/%E4%BA%8C%E5%88%86%E6%90%9C%E7%B4%A2/","section":"Tags","summary":"","title":"二分搜索","type":"tags"},{"content":"","date":"2024年12月24日","externalUrl":null,"permalink":"/zh-cn/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/","section":"Tags","summary":"","title":"动态规划","type":"tags"},{"content":"","date":"2024年12月24日","externalUrl":null,"permalink":"/zh-cn/series/%E8%83%8C%E5%8C%85%E7%BB%8F%E5%85%B8%E9%97%AE%E9%A2%98%E9%9B%86/","section":"Series","summary":"","title":"背包经典问题集","type":"series"},{"content":"","date":"2024年12月24日","externalUrl":null,"permalink":"/zh-cn/tags/%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98/","section":"Tags","summary":"","title":"背包问题","type":"tags"},{"content":"","date":"20 December 2024","externalUrl":null,"permalink":"/tags/cap-theorem/","section":"Tags","summary":"","title":"CAP Theorem","type":"tags"},{"content":"","date":"2024年12月20日","externalUrl":null,"permalink":"/zh-cn/tags/cap-%E5%AE%9A%E7%90%86/","section":"Tags","summary":"","title":"CAP 定理","type":"tags"},{"content":"","date":"20 December 2024","externalUrl":null,"permalink":"/tags/distributed-consensus-algorithm/","section":"Tags","summary":"","title":"Distributed Consensus Algorithm","type":"tags"},{"content":"","date":"20 December 2024","externalUrl":null,"permalink":"/tags/distributed-system/","section":"Tags","summary":"","title":"Distributed System","type":"tags"},{"content":"","date":"20 December 2024","externalUrl":null,"permalink":"/tags/system-design/","section":"Tags","summary":"","title":"System Design","type":"tags"},{"content":"","date":"2024年12月20日","externalUrl":null,"permalink":"/zh-cn/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7%E7%AE%97%E6%B3%95/","section":"Tags","summary":"","title":"分布式一致性算法","type":"tags"},{"content":"","date":"2024年12月20日","externalUrl":null,"permalink":"/zh-cn/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/","section":"Tags","summary":"","title":"分布式系统","type":"tags"},{"content":"","date":"2024年12月20日","externalUrl":null,"permalink":"/zh-cn/tags/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/","section":"Tags","summary":"","title":"系统设计","type":"tags"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"}]