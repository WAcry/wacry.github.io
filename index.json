[{"content":"","date":"30 December 2024","externalUrl":null,"permalink":"/tags/cap-theorem/","section":"Tags","summary":"","title":"CAP Theorem","type":"tags"},{"content":"","date":"2024年12月30日","externalUrl":null,"permalink":"/zh-cn/tags/cap-%E5%AE%9A%E7%90%86/","section":"Tags","summary":"","title":"CAP 定理","type":"tags"},{"content":"","date":"30 December 2024","externalUrl":null,"permalink":"/","section":"David Zhang","summary":"","title":"David Zhang","type":"page"},{"content":" 1. Background: Why is Distributed Consensus Needed? # 1.1 The Consistency Challenge in Distributed Systems # In distributed systems, data and computation are spread across multiple servers, each server (node) connected via a network. This approach enhances system throughput and availability, but it also introduces a core challenge: how to maintain a consistent state or data across multiple nodes.\nConsider a simple example: Suppose we have a distributed key-value storage system. To improve reliability, we need to replicate client write operations across multiple nodes. If a node crashes or a network failure occurs, how do we ensure that the node can obtain the latest state upon recovery? How do we prevent \u0026ldquo;dirty data\u0026rdquo; or inconsistencies? These are the problems that distributed consensus algorithms need to solve.\n1.2 Common Consensus Algorithms # When it comes to distributed consensus, many people first think of Paxos. Paxos has rigorous mathematical proofs, but its implementation and understanding are relatively obscure. In 2014, Raft was proposed as a \u0026ldquo;more readable version\u0026rdquo; of Paxos. It breaks down the consensus process into more understandable steps and is easier to implement while ensuring safety. It is widely used in industrial-grade projects such as etcd, Consul, and TiKV.\n1.3 Raft\u0026rsquo;s Design Goals # Easy to understand: Compared to Paxos, Raft strives for simplicity and clarity in both abstraction and implementation. Strong consistency: Multiple nodes eventually agree on the order and content of log entries. High availability: The system can continue to provide services as long as a majority of nodes are alive. Scalability: Supports dynamically adding or removing nodes (configuration changes). 1.4 Raft Visualization # 2. Raft\u0026rsquo;s Core Ideas # 2.1 Process Overview # Raft breaks down the consensus problem into three sub-problems:\nLeader Election: How to elect a \u0026ldquo;Leader\u0026rdquo; from multiple nodes. Log Replication: After the Leader receives a write request, how to safely replicate the log to a majority of nodes. Safety: When the Leader crashes or a network partition occurs, how to ensure that the system does not experience log loss or overwriting. In Raft, nodes have three roles:\nLeader: Handles client requests and replicates them to other Followers; monitors failures. Follower: Passively receives log replication RPCs and heartbeats from the Leader. Candidate: A role that competes to become the new Leader during the election phase. The system\u0026rsquo;s timeline is divided into a series of Terms. There can be at most one legitimate Leader in the same term. If a Leader cannot be successfully elected in the current term or the Leader crashes, the system enters the next term to re-elect.\n2.2 Data Structures # When implementing Raft, it is usually necessary to maintain:\nTerm: A monotonically increasing number that identifies the current term. Log: Stores client requests or commands. Each log entry includes index, term, and command. commitIndex: The highest log index that has been committed and is visible to the outside world. lastApplied: The highest log index that this node has applied to its local state machine, $\\text{lastApplied} \\leq \\text{commitIndex}$. nextIndex[]: Maintained in the Leader, it records the next log index that needs to be sent to each Follower. matchIndex[]: Maintained in the Leader, it records the highest log index that each Follower has replicated. 3. Roles and State Transitions # 3.1 Follower # Initial State: Nodes are in the Follower state when the system starts; or when a candidate election fails and returns to Follower. Behavior: Receives and responds to the Leader\u0026rsquo;s log replication RPCs, heartbeats, and vote requests from candidates. If it does not receive any messages from the Leader or candidates within the election timeout period, it transitions to the Candidate state. 3.2 Candidate # Trigger Condition: After a Follower times out, it becomes a Candidate and initiates an election. Main Behaviors: Increments currentTerm by 1, and votes for itself (voteFor = self). Sends \u0026ldquo;Request Vote\u0026rdquo; RPCs to other nodes in parallel. If it receives a majority of votes, it becomes the Leader; if a Leader with a newer term appears, it returns to Follower; if a Leader is not elected before the timeout, it starts the next round of elections. 3.3 Leader # Trigger Condition: A Candidate receives a majority of votes. Main Behaviors: Periodically sends heartbeats (AppendEntries RPC) to all nodes to prevent them from initiating elections. Processes client requests, encapsulates them into log entries, writes them to its own log, and then broadcasts them to other Followers. That is, in a strongly consistent scenario, Followers/Candidates are not responsible for handling read/write operations, but only serve as redundancy for the Leader to enhance availability. When a majority of nodes have replicated a log entry, it updates commitIndex and notifies Followers to commit. Is responsible for updating the cluster configuration (adding/removing nodes) and committing the corresponding log entries in a safe manner. 4. Leader Election Process # Raft\u0026rsquo;s election process can be summarized as follows:\nFollower Timeout If a Follower does not receive a heartbeat or log replication message from the Leader within the election timeout period, it switches to the Candidate state.\nCandidate Initiates Election\ncurrentTerm++ voteFor = candidateID Resets the election timer Sends \u0026ldquo;Request Vote\u0026rdquo; RPCs to all nodes, including: $\\langle \\text{candidateTerm}, \\text{candidateId}, \\text{lastLogIndex}, \\text{lastLogTerm} \\rangle$ Node Responds to Vote If the following conditions are met, the node votes for the requester:\nRequester\u0026rsquo;s term number \u0026gt;= this node\u0026rsquo;s term number This node has not voted in this term Requester\u0026rsquo;s log is not older than its own (Term is newer or Index is greater) Otherwise, it refuses to vote.\nElection Completion\nIf the Candidate receives a majority of votes =\u0026gt; becomes the Leader. If it does not receive a majority of votes before the timeout =\u0026gt; enters the next round of elections (Term is incremented by 1 again). If it receives a message from a Leader with a higher term during this period =\u0026gt; returns to Follower. Leader Initialization After becoming the Leader:\nInitializes nextIndex[] = (Leader's latest log index + 1) Initializes matchIndex[] = 0 Immediately broadcasts a heartbeat to announce its leadership. 5. Log Replication # 5.1 AppendEntries RPC # This is the core RPC for Raft log replication and heartbeats. The fields of the request and response are as follows:\nRequest: $ \\langle \\text{term}, \\text{leaderId}, \\text{prevLogIndex}, \\text{prevLogTerm}, \\text{entries[]}, \\text{leaderCommit} \\rangle$ When entries[] is empty, it acts as a heartbeat. prevLogIndex and prevLogTerm are used to match the Follower\u0026rsquo;s log. If there is a mismatch, replication fails. Response: $\\langle \\text{term}, \\text{success} \\rangle$ term: The Follower\u0026rsquo;s current term (if it is greater than the requester\u0026rsquo;s, the requester updates its term and returns to Follower). success: Whether the log was successfully matched and appended. 5.2 Core Log Replication Logic # Leader Writes to Local Log First: Upon receiving a client write request, the Leader first forms a new log entry [index, term, command] and writes it to its own log. Leader Sends AppendEntries: The Leader sends this new log entry to all Followers in parallel. Follower Checks and Matches: If prevLogIndex/prevLogTerm do not match, it returns success = false, and the Leader will roll back and retry. Commit: When the Leader finds that a majority of nodes have replicated a log entry at index N, it updates commitIndex to N and informs the Followers in subsequent heartbeats that they \u0026ldquo;can commit up to N\u0026rdquo;, thereby applying the log to the state machine. 5.3 Important Guarantees: Log Consistency # Log Matching Property: Same index, same term =\u0026gt; all preceding logs are consistent; ensures that there will be no different commands at the same position. Leader Completeness Property: A log entry committed in term $t$ must exist in the log of any Leader elected in any subsequent term; ensures that committed log entries are not discarded or overwritten. 6. Safety # 6.1 Why Raft Can Guarantee Consistency # Majority Mechanism: Decisions (such as committing logs) require agreement from a majority of nodes, tolerating a small number of failures. Term Mechanism: Only trust messages from a higher term, avoiding cross-term log conflicts. Log Matching: Followers must match the previous log entry before appending a new log entry, preventing forks. 6.2 Leader Crashes and Safe Log Inheritance # After a Leader crashes, Followers time out and enter the Candidate state, initiating a new term election. If a new Leader is elected, its log must be the \u0026ldquo;newest\u0026rdquo; in the cluster (because the voting rules require a majority of nodes to agree); therefore, committed logs will not be discarded. Log entries that have not been replicated to a majority of nodes may be overwritten by the new Leader, but this is the correct \u0026ldquo;safe behavior\u0026rdquo; because these logs have not yet reached consensus in the cluster. 7. Implementation Key Points and Difficulties # 7.1 Election Timeout # The election timeout for each node is randomized within the range of [150ms, 300ms] (or a larger range). This reduces conflicts caused by \u0026ldquo;simultaneous elections\u0026rdquo;. 7.2 Heartbeat Interval # The Leader needs to periodically (e.g., every 50~100ms) send heartbeats to Followers to reset their election timeouts. Heartbeats are also empty AppendEntries RPCs, just without any log entries. 7.3 Log Conflict Handling # When a Follower\u0026rsquo;s log does not match the Leader\u0026rsquo;s, it needs to roll back to the matching position. Raft provides several optimization schemes (such as maintaining richer index information in the log) to reduce repeated retries of RPCs. 7.4 Persistence # Raft nodes need to persist: currentTerm, voteFor, and the written logs. Whenever the term or log is updated, it should first be written to the disk log before returning success, so that the node can recover its state after a crash and restart. Raft usually uses sequential log writing + on-demand truncation + snapshots to improve disk write performance. 7.5 State Machine # Committed logs are applied to the state machine, and the specific logic is defined by the user (e.g., key-value storage, database operations, etc.). Raft ensures that the log order is consistent, but the specific execution process takes place in the state machine. 7.6 Cluster Configuration Changes # In actual engineering, it is often necessary to add or remove nodes. Raft uses a \u0026ldquo;two-phase\u0026rdquo; configuration change mechanism: old configuration -\u0026gt; joint configuration (including old and new nodes) -\u0026gt; new configuration, to prevent majority confusion during configuration transitions. 8. Core Properties of Raft Correctness Proof # 8.1 Distributed Environment Assumptions and Safety/Liveness # Raft assumes a partially synchronous network: that is, the network is synchronous most of the time (with a stable maximum delay), but long delays or partitions may occasionally occur. As long as the network eventually returns to a communicable state, Raft can achieve consensus. Safety: No matter how unstable the network is, there will be no conflicts such as committed logs being rolled back or two nodes applying different commands at the same index. Liveness: As long as the network is in a synchronous state for a sufficient amount of time, Raft will eventually elect a Leader and continuously commit new logs. If the network is always partitioned or messages are lost, even the best distributed algorithm may be blocked. This is the famous FLP (Fischer–Lynch–Paterson) impossibility theorem. 8.2 Main Properties and Overall Approach # Raft relies on the following two core properties to ensure that the system log remains consistent between nodes and does not roll back:\nLog Matching Property \u0026ldquo;If the term numbers at log index $i$ of two nodes are the same, then the entire log segment from the beginning to $i$ is consistent.\u0026rdquo; Leader Completeness Property \u0026ldquo;If a log entry has been committed in term $t$, then any Leader\u0026rsquo;s log in subsequent terms must also contain that log entry.\u0026rdquo; Based on these two points, we can derive State Machine Safety:\nOnce a node applies command $cmd$ at index $i$, no other node will apply a different command at the same index $i$ that is not $cmd$.\n8.3 Log Matching Property # Property Statement: If the term numbers at index $i$ of any two nodes are the same, then the log entries from 1 to $i$ of the two nodes are completely the same (both term and command are consistent).\nProof Approach: Raft\u0026rsquo;s replication rules require a Follower to match prevLogIndex \u0026amp; prevLogTerm before appending a new log entry; otherwise, it returns failure and causes the Leader to roll back. Therefore, only when their preceding logs are completely consistent can they write the same (index, term). From this, we can inductively conclude that the same position and the same term must have the same preceding logs. 8.4 Leader Completeness Property # Property Statement: If a log entry is committed (written to a majority of nodes) in term $t$, then any legitimate Leader in subsequent terms must contain this log entry.\nProof Approach: A log entry being committed means that a majority has written it; a new Leader being elected requires a majority vote; when voting, each node will refuse to vote for a candidate whose log is \u0026ldquo;behind\u0026rdquo; it. Therefore, a candidate who can be elected must already contain all committed logs; from this, we can infer that the log committed in term $t$ will not be \u0026ldquo;forgotten\u0026rdquo; or overwritten. 8.5 Election Safety # Proposition: It is impossible for two different nodes to receive a majority of votes and become Leader in the same term $t$.\nCore Reason: In term $t$, each node can only cast 1 vote. If one candidate has already received a majority of votes, it is impossible for another candidate to also receive a majority of votes (because the nodes in the majority have already voted for the former and cannot vote again). 8.6 No Commit Overwritten # Proposition: If a log entry has been committed, it will not be overwritten with a different command or be revoked.\nProof Approach: As long as a log has been written to a majority, in order to overwrite it, it is necessary to obtain votes from these majority nodes and successfully write the log. However, if the majority nodes find that a candidate is \u0026ldquo;missing\u0026rdquo; this log, they will refuse to vote or refuse the matching of AppendEntries. Therefore, it is impossible to form a new majority to overwrite this log. 8.7 State Machine Safety # Proposition: If node A applies command $cmd$ at index $i$, then no node B will apply a different command $cmd' \\neq cmd$ at the same index $i$.\nProof Approach: Applying to the state machine means that the log has been committed; committing means that it has been replicated to a majority, and the subsequent new Leader will also carry this log; it is impossible for a parallel, conflicting log to be written to the same index and be accepted by a majority of nodes. 8.8 Liveness # In a normal partially synchronous network, once a Leader is stable, client requests will be continuously committed. If the Leader crashes or the network is temporarily partitioned, an election will be initiated after recovery, and a new Leader will eventually be elected. In a completely asynchronous or permanently partitioned situation, all distributed consensus protocols cannot guarantee inevitable liveness (FLP theorem), but as long as the network is \u0026ldquo;occasionally stable\u0026rdquo;, Raft can continue to move forward. In summary, Raft\u0026rsquo;s design balances the three major goals of easy to understand, easy to implement, and strong consistency. Through terms, elections, log replication, and majority mechanisms, it ensures the linearizable semantics of data in distributed systems. Its core safety can be summarized as:\nThere will not be two legitimate Leaders at the same time. Committed logs are never rolled back. Committed logs are still retained by the new Leader. Ensures that the same (index, term) position will not have different commands on different nodes. Ensures that logs committed in a certain term will not disappear on subsequent Leaders. 9. Implementing a Minimal Raft from Scratch: Key Steps # Define Data Structures currentTerm, voteFor, log[], commitIndex, lastApplied, etc. Node roles (Follower/Candidate/Leader) Timers, RPC channels, etc. Persistence Layer Write key data such as term, votes, and logs to disk or store in a storage engine such as RocksDB. Load from storage first when restarting. RPC Communication Layer Implement two types of RPCs: RequestVote and AppendEntries. Handle network exceptions and timeout retries. Event Loop \u0026amp; Timers Periodically check if a heartbeat has been received from the Leader. Initiate an election if a timeout occurs. If it is a Leader, send heartbeats periodically. Process network RPCs and update local state. Leader Election Follower timeout -\u0026gt; Candidate initiates voting -\u0026gt; If a majority is obtained -\u0026gt; Leader If it fails or encounters a Leader with a higher term -\u0026gt; returns to Follower Log Replication Leader receives a new command -\u0026gt; writes to local log -\u0026gt; AppendEntries to Followers Majority of nodes replicate -\u0026gt; Leader commits and notifies Followers to commit Fault Recovery After a node restarts, it first restores its state from storage. After a Leader or network failure, a new Leader will continue to be elected through the election timeout mechanism. Testing and Verification Simulate network delays, partitions, node failures, etc., to check whether Raft can correctly maintain consistency and external availability. Code Example: Minimal Raft in C# # 1using System; 2using System.Collections.Generic; 3using System.Linq; 4using System.Threading; 5using System.Threading.Tasks; 6 7// This code example focuses on the core election logic of the Raft algorithm, omitting details such as network communication and persistence. 8namespace RaftDemo 9{ 10 #region Data Structures 11 12 public enum NodeRole 13 { 14 Follower, 15 Candidate, 16 Leader 17 } 18 19 public class LogEntry 20 { 21 public int Index { get; set; } 22 public int Term { get; set; } 23 public string Command { get; set; } 24 } 25 26 public class RequestVoteRequest 27 { 28 public int Term { get; set; } 29 public string CandidateId { get; set; } 30 public int LastLogIndex { get; set; } 31 public int LastLogTerm { get; set; } 32 } 33 34 public class RequestVoteResponse 35 { 36 public int Term { get; set; } 37 public bool VoteGranted { get; set; } 38 } 39 40 public class AppendEntriesRequest 41 { 42 public int Term { get; set; } 43 public string LeaderId { get; set; } 44 public int PrevLogIndex { get; set; } 45 public int PrevLogTerm { get; set; } 46 public List\u0026lt;LogEntry\u0026gt; Entries { get; set; } = new List\u0026lt;LogEntry\u0026gt;(); 47 public int LeaderCommit { get; set; } 48 } 49 50 public class AppendEntriesResponse 51 { 52 public int Term { get; set; } 53 public bool Success { get; set; } 54 // To simplify, no additional rollback information is carried here; if more information is included, nextIndex can be rolled back more quickly to find the matching position, reducing the number of network interactions. 55 } 56 57 public interface IRaftRpc 58 { 59 RequestVoteResponse RequestVote(RequestVoteRequest request); 60 AppendEntriesResponse AppendEntries(AppendEntriesRequest request); 61 } 62 63 #endregion 64 65 #region RaftNode Core Class 66 67 public class RaftNode : IRaftRpc 68 { 69 private readonly string _nodeId; 70 private readonly InMemoryDispatcher _dispatcher; 71 private readonly Random _rand = new Random(); 72 73 private NodeRole _role = NodeRole.Follower; 74 private int _currentTerm = 0; 75 private string _votedFor = null; 76 77 private List\u0026lt;LogEntry\u0026gt; _log = new List\u0026lt;LogEntry\u0026gt;(); 78 private int _commitIndex = 0; 79 private int _lastApplied = 0; 80 81 // Only Leaders need to maintain these 82 private Dictionary\u0026lt;string, int\u0026gt; _nextIndex = new Dictionary\u0026lt;string, int\u0026gt;(); 83 private Dictionary\u0026lt;string, int\u0026gt; _matchIndex = new Dictionary\u0026lt;string, int\u0026gt;(); 84 85 private CancellationTokenSource _cts; 86 private Task _timerTask; 87 88 // The state machine in the demo uses a simple list to record commands 89 private List\u0026lt;string\u0026gt; _stateMachine = new List\u0026lt;string\u0026gt;(); 90 91 public RaftNode(string nodeId, InMemoryDispatcher dispatcher) 92 { 93 _nodeId = nodeId; 94 _dispatcher = dispatcher; 95 } 96 97 public void Start() 98 { 99 _dispatcher.RegisterNode(_nodeId, this); 100 ResetTimer(); 101 } 102 103 public void Stop() 104 { 105 _cts?.Cancel(); 106 } 107 108 // For testing, input commands from external sources 109 public void SendClientCommand(string command) 110 { 111 if (_role != NodeRole.Leader) 112 { 113 Console.WriteLine($\u0026#34;[{_nodeId}] I\u0026#39;m not the leader, ignoring client command.\u0026#34;); 114 return; 115 } 116 117 // 1. Append the command to the local log 118 var newLogIndex = LastLogIndex() + 1; 119 _log.Add(new LogEntry 120 { 121 Index = newLogIndex, 122 Term = _currentTerm, 123 Command = command 124 }); 125 126 Console.WriteLine($\u0026#34;[{_nodeId}] Leader received client command \u0026#39;{command}\u0026#39;, logIndex={newLogIndex}.\u0026#34;); 127 128 // 2. Asynchronously send AppendEntries to other nodes 129 BroadcastAppendEntries(); 130 } 131 132 #region Core RPC Implementation 133 134 public RequestVoteResponse RequestVote(RequestVoteRequest request) 135 { 136 lock (this) 137 { 138 // If the term in the request is higher than the local term, update the local term and become a Follower 139 if (request.Term \u0026gt; _currentTerm) 140 { 141 _currentTerm = request.Term; 142 _role = NodeRole.Follower; 143 _votedFor = null; 144 } 145 146 var response = new RequestVoteResponse 147 { 148 Term = _currentTerm, 149 VoteGranted = false 150 }; 151 152 // If the term in the request is less than the local term, reject the vote 153 if (request.Term \u0026lt; _currentTerm) 154 { 155 return response; 156 } 157 158 // Check if already voted for another candidate or if the log is up-to-date 159 var myLastLogIndex = LastLogIndex(); 160 var myLastLogTerm = LastLogTerm(); 161 162 bool logIsAtLeastUpToDate = 163 (request.LastLogTerm \u0026gt; myLastLogTerm) || 164 (request.LastLogTerm == myLastLogTerm \u0026amp;\u0026amp; request.LastLogIndex \u0026gt;= myLastLogIndex); 165 166 if ((_votedFor == null || _votedFor == request.CandidateId) \u0026amp;\u0026amp; logIsAtLeastUpToDate) 167 { 168 _votedFor = request.CandidateId; 169 response.VoteGranted = true; 170 // Reset the election timeout 171 ResetTimer(); 172 } 173 174 return response; 175 } 176 } 177 178 public AppendEntriesResponse AppendEntries(AppendEntriesRequest request) 179 { 180 lock (this) 181 { 182 var response = new AppendEntriesResponse 183 { 184 Term = _currentTerm, 185 Success = false 186 }; 187 188 // If the term is less than the local term, reject the request 189 if (request.Term \u0026lt; _currentTerm) 190 { 191 return response; 192 } 193 194 // If the term is higher than the local term, update the term and become a Follower 195 if (request.Term \u0026gt; _currentTerm) 196 { 197 _currentTerm = request.Term; 198 _role = NodeRole.Follower; 199 _votedFor = null; 200 } 201 202 // Received a heartbeat/log replication from the Leader, reset the election timeout 203 ResetTimer(); 204 205 // Simplified basic checks: 206 // Check if the local log at prevLogIndex has the same term 207 if (request.PrevLogIndex \u0026gt; 0) 208 { 209 if (request.PrevLogIndex \u0026gt; _log.Count) 210 { 211 // Does not match (local log is shorter than the Leader\u0026#39;s) 212 return response; 213 } 214 else 215 { 216 var localTerm = _log[request.PrevLogIndex - 1].Term; 217 if (localTerm != request.PrevLogTerm) 218 { 219 // Does not match (term conflict) 220 return response; 221 } 222 } 223 } 224 225 // If it matches, continue: first overwrite any conflicting logs 226 foreach (var entry in request.Entries) 227 { 228 if (entry.Index \u0026lt;= _log.Count) 229 { 230 // Existing log at the same index, check for conflict 231 if (_log[entry.Index - 1].Term != entry.Term) 232 { 233 _log[entry.Index - 1] = entry; 234 Console.WriteLine($\u0026#34;[{_nodeId}] Overwritten log at index {entry.Index} with term {entry.Term}.\u0026#34;); 235 } 236 } 237 else 238 { 239 _log.Add(entry); 240 Console.WriteLine($\u0026#34;[{_nodeId}] Appended log at index {entry.Index} with term {entry.Term}.\u0026#34;); 241 } 242 } 243 244 // Update commitIndex 245 if (request.LeaderCommit \u0026gt; _commitIndex) 246 { 247 _commitIndex = Math.Min(request.LeaderCommit, _log.Count); 248 ApplyStateMachine(); 249 } 250 251 response.Success = true; 252 response.Term = _currentTerm; 253 return response; 254 } 255 } 256 257 #endregion 258 259 #region Election and Heartbeat Timers 260 261 private void ResetTimer() 262 { 263 _cts?.Cancel(); 264 _cts = new CancellationTokenSource(); 265 _timerTask = RunTimer(_cts.Token); 266 } 267 268 private async Task RunTimer(CancellationToken token) 269 { 270 // In a production environment, the election timeout is typically set to around 150~300ms, 271 // here it\u0026#39;s extended to 1.5~3 seconds for demonstration purposes, making it easier to observe log outputs in the Console. 272 var electionTimeout = _rand.Next(1500, 3000); // Random between 1.5~3s 273 var heartbeatInterval = 500; // Leader heartbeat interval 0.5s 274 275 var start = DateTime.UtcNow; 276 while (!token.IsCancellationRequested) 277 { 278 await Task.Delay(100, token); 279 var elapsed = (DateTime.UtcNow - start).TotalMilliseconds; 280 281 if (_role == NodeRole.Leader) 282 { 283 // Leader periodically sends heartbeats 284 if (elapsed \u0026gt;= heartbeatInterval) 285 { 286 BroadcastAppendEntries(); 287 start = DateTime.UtcNow; 288 } 289 } 290 else 291 { 292 // Follower or Candidate, need to check if election should be triggered 293 if (elapsed \u0026gt;= electionTimeout) 294 { 295 BecomeCandidate(); 296 start = DateTime.UtcNow; 297 } 298 } 299 } 300 } 301 302 private void BecomeCandidate() 303 { 304 lock (this) 305 { 306 _role = NodeRole.Candidate; 307 _currentTerm++; 308 _votedFor = _nodeId; 309 310 Console.WriteLine($\u0026#34;[{_nodeId}] Timeout -\u0026gt; Candidate. term={_currentTerm}\u0026#34;); 311 312 // Reset the election timeout 313 ResetTimer(); 314 } 315 316 // Initiate voting 317 var request = new RequestVoteRequest 318 { 319 Term = _currentTerm, 320 CandidateId = _nodeId, 321 LastLogIndex = LastLogIndex(), 322 LastLogTerm = LastLogTerm() 323 }; 324 325 var votesGranted = 1; // Vote for self 326 var totalNodes = _dispatcher.GetAllNodes().Count; 327 var majority = totalNodes / 2 + 1; 328 329 foreach (var otherId in _dispatcher.GetAllNodes().Where(id =\u0026gt; id != _nodeId)) 330 { 331 Task.Run(() =\u0026gt; 332 { 333 var resp = _dispatcher.SendRequestVote(otherId, request); 334 if (resp != null) 335 { 336 lock (this) 337 { 338 // If the other node\u0026#39;s term is higher, become a Follower 339 if (resp.Term \u0026gt; _currentTerm) 340 { 341 _currentTerm = resp.Term; 342 _role = NodeRole.Follower; 343 _votedFor = null; 344 return; 345 } 346 347 if (resp.VoteGranted \u0026amp;\u0026amp; _role == NodeRole.Candidate \u0026amp;\u0026amp; resp.Term == _currentTerm) 348 { 349 votesGranted++; 350 if (votesGranted \u0026gt;= majority) 351 { 352 BecomeLeader(); 353 } 354 } 355 } 356 } 357 }); 358 } 359 } 360 361 private void BecomeLeader() 362 { 363 if (_role != NodeRole.Candidate) return; 364 365 _role = NodeRole.Leader; 366 Console.WriteLine($\u0026#34;[{_nodeId}] Became Leader at term {_currentTerm}!\u0026#34;); 367 368 // Initialize nextIndex and matchIndex 369 foreach (var node in _dispatcher.GetAllNodes()) 370 { 371 if (node == _nodeId) continue; 372 _nextIndex[node] = LastLogIndex() + 1; 373 _matchIndex[node] = 0; 374 } 375 376 // Immediately send a heartbeat 377 BroadcastAppendEntries(); 378 } 379 380 private void BroadcastAppendEntries() 381 { 382 lock (this) 383 { 384 if (_role != NodeRole.Leader) return; 385 386 foreach (var otherId in _dispatcher.GetAllNodes().Where(id =\u0026gt; id != _nodeId)) 387 { 388 var prevLogIndex = _nextIndex[otherId] - 1; 389 var prevLogTerm = (prevLogIndex \u0026gt; 0 \u0026amp;\u0026amp; prevLogIndex \u0026lt;= _log.Count) 390 ? _log[prevLogIndex - 1].Term : 0; 391 392 // Simplified: send only a limited set of logs each time 393 // Can send more as needed 394 var entriesToSend = new List\u0026lt;LogEntry\u0026gt;(); 395 if (prevLogIndex \u0026lt; _log.Count) 396 { 397 entriesToSend = _log.Skip(prevLogIndex).ToList(); 398 } 399 400 var request = new AppendEntriesRequest 401 { 402 Term = _currentTerm, 403 LeaderId = _nodeId, 404 PrevLogIndex = prevLogIndex, 405 PrevLogTerm = prevLogTerm, 406 Entries = entriesToSend, 407 LeaderCommit = _commitIndex 408 }; 409 410 Task.Run(() =\u0026gt; 411 { 412 var resp = _dispatcher.SendAppendEntries(otherId, request); 413 if (resp != null) 414 { 415 lock (this) 416 { 417 if (resp.Term \u0026gt; _currentTerm) 418 { 419 _currentTerm = resp.Term; 420 _role = NodeRole.Follower; 421 _votedFor = null; 422 return; 423 } 424 425 if (resp.Success) 426 { 427 // Indicates the other node has matched prevLogIndex 428 // Update nextIndex \u0026amp; matchIndex 429 var lastEntry = entriesToSend.LastOrDefault(); 430 if (lastEntry != null) 431 { 432 _matchIndex[otherId] = lastEntry.Index; 433 _nextIndex[otherId] = lastEntry.Index + 1; 434 } 435 436 // Update commitIndex 437 UpdateCommitIndex(); 438 } 439 else 440 { 441 // Decrement nextIndex and retry (simple backoff strategy) 442 _nextIndex[otherId] = Math.Max(1, _nextIndex[otherId] - 1); 443 } 444 } 445 } 446 }); 447 } 448 } 449 } 450 451 private void UpdateCommitIndex() 452 { 453 // Leader updates commitIndex based on matchIndex 454 // Find an N such that N \u0026gt; current commitIndex, a majority of matchIndex \u0026gt;= N, and _log[N].Term == currentTerm 455 for (int i = _log.Count; i \u0026gt; _commitIndex; i--) 456 { 457 var count = _matchIndex.Values.Count(m =\u0026gt; m \u0026gt;= i) + 1; // +1 includes self 458 if (count \u0026gt;= (_dispatcher.GetAllNodes().Count / 2 + 1) \u0026amp;\u0026amp; _log[i - 1].Term == _currentTerm) 459 { 460 _commitIndex = i; 461 ApplyStateMachine(); 462 break; 463 } 464 } 465 } 466 467 private void ApplyStateMachine() 468 { 469 // Apply logs from [lastApplied+1 ... commitIndex] to the state machine 470 while (_lastApplied \u0026lt; _commitIndex) 471 { 472 _lastApplied++; 473 var cmd = _log[_lastApplied - 1].Command; 474 _stateMachine.Add(cmd); 475 Console.WriteLine($\u0026#34;[{_nodeId}] Apply LogIndex={_lastApplied}, Cmd={cmd}\u0026#34;); 476 } 477 } 478 479 #endregion 480 481 #region Helper Methods 482 483 private int LastLogIndex() 484 { 485 return _log.Count; 486 } 487 488 private int LastLogTerm() 489 { 490 if (_log.Count == 0) return 0; 491 return _log[_log.Count - 1].Term; 492 } 493 494 public NodeRole Role =\u0026gt; _role; 495 public int CurrentTerm =\u0026gt; _currentTerm; 496 public List\u0026lt;string\u0026gt; StateMachine =\u0026gt; _stateMachine; 497 498 #endregion 499 } 500 501 #endregion 502 503 #region Simple \u0026#34;RPC Dispatcher\u0026#34; to Simulate Network 504 505 public class InMemoryDispatcher 506 { 507 private Dictionary\u0026lt;string, IRaftRpc\u0026gt; _nodes = new Dictionary\u0026lt;string, IRaftRpc\u0026gt;(); 508 509 public void RegisterNode(string nodeId, IRaftRpc node) 510 { 511 _nodes[nodeId] = node; 512 } 513 514 public List\u0026lt;string\u0026gt; GetAllNodes() =\u0026gt; _nodes.Keys.ToList(); 515 516 public RequestVoteResponse SendRequestVote(string targetNodeId, RequestVoteRequest request) 517 { 518 if (!_nodes.ContainsKey(targetNodeId)) return null; 519 return _nodes[targetNodeId].RequestVote(request); 520 } 521 522 public AppendEntriesResponse SendAppendEntries(string targetNodeId, AppendEntriesRequest request) 523 { 524 if (!_nodes.ContainsKey(targetNodeId)) return null; 525 return _nodes[targetNodeId].AppendEntries(request); 526 } 527 } 528 529 #endregion 530 531 #region Program Demo 532 533 class Program 534 { 535 static void Main(string[] args) 536 { 537 var dispatcher = new InMemoryDispatcher(); 538 539 var node1 = new RaftNode(\u0026#34;Node1\u0026#34;, dispatcher); 540 var node2 = new RaftNode(\u0026#34;Node2\u0026#34;, dispatcher); 541 var node3 = new RaftNode(\u0026#34;Node3\u0026#34;, dispatcher); 542 543 node1.Start(); 544 node2.Start(); 545 node3.Start(); 546 547 Console.WriteLine(\u0026#34;Raft cluster started with 3 nodes (Node1, Node2, Node3).\u0026#34;); 548 Console.WriteLine(\u0026#34;Wait a few seconds to see leader election...\u0026#34;); 549 550 // Wait a few seconds to allow election to complete 551 Task.Delay(5000).Wait(); 552 553 // Attempt to send a command to node1; if it\u0026#39;s the Leader, it will execute, otherwise it will indicate it\u0026#39;s not the Leader 554 node1.SendClientCommand(\u0026#34;SET X=100\u0026#34;); 555 556 Task.Delay(3000).Wait(); 557 558 // Stop node2 to simulate a crash 559 Console.WriteLine(\u0026#34;\\n** Stopping Node2 to simulate crash **\\n\u0026#34;); 560 node2.Stop(); 561 562 // Wait for a while before sending another command 563 Task.Delay(5000).Wait(); 564 node1.SendClientCommand(\u0026#34;SET Y=200\u0026#34;); 565 566 // Wait again to allow log replication to complete 567 Task.Delay(5000).Wait(); 568 569 // Print the data applied to the state machine of each node 570 Console.WriteLine($\u0026#34;\\nNode1 state: {string.Join(\u0026#34;, \u0026#34;, node1.StateMachine)}\u0026#34;); 571 Console.WriteLine($\u0026#34;\\nNode2 state: {string.Join(\u0026#34;, \u0026#34;, node2.StateMachine)} (Should not receive new logs after crash)\u0026#34;); 572 Console.WriteLine($\u0026#34;\\nNode3 state: {string.Join(\u0026#34;, \u0026#34;, node3.StateMachine)}\u0026#34;); 573 574 // Shutdown 575 node1.Stop(); 576 node2.Stop(); 577 node3.Stop(); 578 579 Console.WriteLine(\u0026#34;\\nDemo finished. Press any key to exit.\u0026#34;); 580 Console.ReadKey(); 581 } 582 } 583 584 #endregion 585} ","date":"30 December 2024","externalUrl":null,"permalink":"/posts/1735548655536-raft-intro/","section":"Posts","summary":"Detailed discussion of the simplest implementation details of the Raft algorithm, code examples, and correctness proofs.","title":"Detailed explanation of Raft algorithm","type":"posts"},{"content":"","date":"30 December 2024","externalUrl":null,"permalink":"/tags/distributed-consensus-algorithm/","section":"Tags","summary":"","title":"Distributed Consensus Algorithm","type":"tags"},{"content":"","date":"30 December 2024","externalUrl":null,"permalink":"/tags/distributed-system/","section":"Tags","summary":"","title":"Distributed System","type":"tags"},{"content":"","date":"30 December 2024","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"30 December 2024","externalUrl":null,"permalink":"/tags/raft-algorithm/","section":"Tags","summary":"","title":"Raft Algorithm","type":"tags"},{"content":"","date":"2024年12月30日","externalUrl":null,"permalink":"/zh-cn/tags/raft-%E7%AE%97%E6%B3%95/","section":"Tags","summary":"","title":"Raft 算法","type":"tags"},{"content":"","date":"30 December 2024","externalUrl":null,"permalink":"/tags/system-design/","section":"Tags","summary":"","title":"System Design","type":"tags"},{"content":"","date":"30 December 2024","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"2024年12月30日","externalUrl":null,"permalink":"/zh-cn/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7%E7%AE%97%E6%B3%95/","section":"Tags","summary":"","title":"分布式一致性算法","type":"tags"},{"content":"","date":"2024年12月30日","externalUrl":null,"permalink":"/zh-cn/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/","section":"Tags","summary":"","title":"分布式系统","type":"tags"},{"content":"","date":"2024年12月30日","externalUrl":null,"permalink":"/zh-cn/tags/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/","section":"Tags","summary":"","title":"系统设计","type":"tags"},{"content":" I. The CAP Theorem # 1.1 What is the CAP Theorem? # The CAP Theorem, proposed by Eric Brewer in 2000, states that in the design of distributed systems, a maximum of two out of the three properties—Consistency (C), Availability (A), and Partition Tolerance (P)—can be satisfied simultaneously.\nC (Consistency): All nodes in the system see the same data at the same time. More strictly, when a client reads data, the result should be consistent with the latest committed data, regardless of which replica it reads from (usually referring to strong/linear consistency). A (Availability): The system can still provide normal service when partial failures occur, and each request can receive a valid response within a reasonable time. P (Partition Tolerance): The system can tolerate network partitions (unreachable communication between nodes). Even if the network is split, the system can provide a certain degree of availability or consistency. In real distributed environments, network partitions are inevitable, so P is generally regarded as a \u0026ldquo;must-have\u0026rdquo;. When a network partition occurs, the system cannot simultaneously ensure both strong consistency and high availability of data across all nodes. It must choose between C and A, resulting in two main types: CP and AP.\n1.2 Limitations of the CAP Theorem # It is important to note that the CAP Theorem itself is a relatively high-level theory, used for conceptual guidance, and should not be simply interpreted as \u0026ldquo;either choose C or choose A\u0026rdquo;. There are some common misconceptions:\nC does not necessarily mean strong consistency The C in the CAP Theorem often refers to the strictest sense of consistency (i.e., linearizability). However, in real systems, there are many fine-grained models to choose from, such as weak consistency, Read Committed, and Causal Consistency. Availability is not binary (0 or 1) It is not the case that choosing CP means availability is completely sacrificed, or that choosing AP means consistency is completely unguaranteed. There is room for trade-offs and degradation strategies for both availability and consistency. Eventual consistency does not violate CAP It is a very common compromise, trading lower write consistency for higher availability and throughput, and converging data in the background asynchronously. Therefore, the CAP Theorem should be combined with various consistency models and high-availability architecture patterns in specific scenarios to provide real practical guidance.\nII. Consistency Models in Distributed Systems # There are many classifications of consistency models, but the common mainstream models can be broadly divided into: Strong Consistency and Weak Consistency (which includes eventual consistency, causal consistency, etc.). This article mainly introduces Strong Consistency and Eventual Consistency, and explains their common applications in CP or AP modes.\n2.1 Strong Consistency # Strong Consistency, also known as Linearizability, means that once a write operation completes and returns successfully, any subsequent read operation will be able to read the updated content. That is, the system behaves as if all operations were executed serially.\nCommon Implementation: Relies on synchronous replication and a quorum (majority) mechanism, using protocols (such as Paxos/Raft) to ensure that there is only one valid leader in the system. All operations are written to the log in order and replicated to the majority of nodes. Advantages and Disadvantages: Advantages: Guarantees the strictest data correctness, and the data read at any time will not \u0026ldquo;revert\u0026rdquo;. Disadvantages: In the event of network jitter, partitions, or leader failures, write operations are often blocked to maintain consistency, leading to a decrease in overall availability; performance and throughput are also relatively lower. 2.2 Eventual Consistency # Eventual Consistency is a typical form of weak consistency. It only requires that if there are no new update operations in the system, the data of all replicas will gradually converge to the same state over time. During this period, users may see outdated values when reading replica data, but it will eventually become consistent.\nCommon Implementation: Gossip protocol, asynchronous multi-replica replication, CRDT (Conflict-free Replicated Data Type), etc. Advantages and Disadvantages: Advantages: High availability, high throughput, low write operation latency, and high tolerance for network partitions. Disadvantages: Requires tolerance for short-term data inconsistencies, more complex application logic, and may require conflict detection and merging. III. Common Consistency Protocols and Algorithms # To maintain consistency between replicas in a distributed system, the industry has proposed many classic algorithms and protocols. Here is a brief introduction to several of them:\n3.1 Paxos # Paxos is a distributed consensus algorithm proposed by Leslie Lamport in the 1990s, mainly used to achieve strong or linear consistency.\nBasic Principle: Through role division (Proposer, Acceptor, Learner), multiple rounds of voting are conducted to determine whether an operation or value is accepted by the majority of nodes. Advantages and Disadvantages: Advantages: Can still reach a consensus under network partitions and node failures, with high security. Disadvantages: Complex implementation, difficult to debug and troubleshoot, and performance is limited due to multiple rounds of voting. Its variants (Multi-Paxos, etc.) are more commonly used in the industry. 3.2 Raft # Raft was officially proposed in 2013, with the goal of simplifying the implementation and understanding while ensuring the same level of security as Paxos. It establishes a stable Leader role, centrally performing log replication and fault recovery:\nKey Stages: Leader Election, Log Replication, Safety, etc. Common Applications: Etcd, Consul, TiKV, LogCabin, etc., are all based on Raft to implement strong consistency replication. Advantages and Disadvantages: Advantages: Relatively easy to understand, less implementation code; better performance for small and medium-sized clusters. Disadvantages: Relies on the master node (Leader), and leader failures or partitions can cause short-term write blocking; in large-scale clusters or cross-regional deployments, latency and availability can be affected. 3.3 Gossip Protocol # The Gossip protocol is not a traditional consensus protocol. It is mainly used in decentralized scenarios to exchange metadata or status information through random node interactions, thereby achieving diffusion and convergence across the entire network.\nFeatures: Decentralized, low overhead, nodes exchange messages periodically and randomly. Common Applications: Cassandra, Riak, distributed membership management (such as Serf), etc., are used to achieve eventual consistency, replica state synchronization, etc. Advantages and Disadvantages: Advantages: Good scalability, simple to implement, suitable for scenarios with low consistency requirements and high scalability requirements. Disadvantages: Weak consistency guarantee, requiring higher-level conflict handling methods (such as CRDT, version number merging, etc.) to ultimately resolve conflicts. 3.4 2PC / 3PC # In distributed transaction scenarios, common commit protocols are 2PC (Two-phase Commit) and 3PC (Three-phase Commit):\n2PC: The coordinator notifies all participants to \u0026ldquo;prepare,\u0026rdquo; and if all are successful, it broadcasts \u0026ldquo;commit,\u0026rdquo; otherwise \u0026ldquo;abort.\u0026rdquo; 3PC: Adds a phase on top of 2PC to reduce blocking caused by single points of failure, but is more complex to implement and still has unavailability issues in extreme network partitions or failure scenarios. Advantages and Disadvantages: Advantages: Easy to understand, clear transaction semantics, widely used in distributed databases, message queues, etc. Disadvantages: Strong dependency on the coordinator, risk of blocking; transactions may not be able to proceed when the network has a long partition. IV. Two Main Choices of CAP: CP and AP # When we consider P as a \u0026ldquo;must-have\u0026rdquo; property, if a distributed system wants to continue providing services during network partitions, it must choose between C and A. Common system designs are therefore divided into two major camps: CP and AP.\n4.1 CP Systems # CP (Consistency + Partition tolerance): When encountering a network partition, the system will choose to prioritize consistency and sacrifice availability when necessary.\nTypical Implementation: Majority consensus (Paxos, Raft, etc.), requiring more than half of the nodes to be alive and reach a consensus before allowing writes. If a quorum cannot be reached or the master node fails, the system will block or reject write operations to prevent data inconsistencies caused by split-brain scenarios. Common Applications: Zookeeper, Etcd, Consul, distributed lock services, distributed metadata management, etc. Core financial transaction processes, bank accounting systems, and other scenarios with high consistency requirements. Features: Strict data guarantees: Preferring downtime rather than dual-master or data confusion. Sacrificing some availability: During network partitions or failovers, there will be a window of service unavailability or rejection of write operations. 4.2 AP Systems # AP (Availability + Partition tolerance): When encountering a network partition, the system will choose to prioritize availability while relaxing consistency.\nTypical Implementation: Eventual consistency, multi-master replication, Gossip protocol, Dynamo-style tunable consistency policies, etc. Common Applications: NoSQL databases (Cassandra, Riak, DynamoDB, etc.), distributed caching systems (Redis Cluster), etc. Social networks, log collection, recommendation systems, and other businesses that require high availability and high throughput, and have relatively relaxed requirements for data consistency. Features: Even during partitions, all nodes still accept read and write requests, ensuring the system is \u0026ldquo;as available as possible.\u0026rdquo; Data may have short-term inconsistencies, but will gradually converge in the background through asynchronous synchronization and conflict merging. V. How to Choose Between CP and AP? # In real large-scale distributed systems, it is often rare to rely on a single model. Instead, different data or business scenarios are layered to achieve the optimal balance between consistency and availability.\nChoose CP for Core Data Such as user account balances, order payments, financial transaction records, etc., which have extremely high consistency requirements. Tolerating short-term write unavailability caused by network jitter, but not tolerating errors in balances or transaction amounts. Choose AP for Edge or Cached Data Such as caches of product detail pages, user behavior logs, recommendation candidate lists, etc., which have lower consistency requirements. Prioritizing high concurrency and high availability, and tolerating a certain amount of delayed updates or dirty reads. Many internet companies adopt a hybrid architecture: Core transaction processes use CP-style storage (such as distributed relational databases or distributed storage with strong consistency); peripheral businesses or \u0026ldquo;read-heavy\u0026rdquo; scenarios use AP-style storage or caching solutions.\nVI. How CP and AP Achieve High Concurrency and Eventual Consistency # 6.1 How CP Systems Cope with High Concurrency # Although consensus protocols face higher latency and lower throughput when the scale of a single cluster node and the volume of write requests are large, concurrency and scalability can still be improved through the following methods:\nBatch Reads and Writes Pack multiple write operations at the client or middleware layer and write them to the leader node at once, reducing network round trips and protocol rounds. Database Sharding \u0026amp; Multi-Cluster Split data logically or by hash into multiple clusters (sharding), with each cluster still running the CP protocol internally; requests are distributed to different shards through routing or proxy layers. Improve overall concurrency and limit the impact of failures to a single shard. The throughput of a single-shard cluster in a CP system is often 2 to 10 times lower than that of an AP system.\n6.2 How AP Systems Ensure Eventual Consistency # AP systems can usually provide high write throughput and read availability, but they relax consistency. Therefore, consistency convergence needs to be ensured in the background or at the business logic layer:\nVersion Numbers (Vector Clock) or Logical Timestamps Assign a version number (or based on Lamport Clock / Hybrid Clock) to each update operation, and merge or use a timestamp-based winner strategy (Last Write Wins) in conflict scenarios. Gossip Protocol / Anti-entropy Mechanism Nodes periodically exchange the latest data or metadata and merge when conflicts are found. Tunable Consistency Policies Represented by the Dynamo model, clients can configure parameters such as R and W (such as writing to a majority, replica confirmation), thereby flexibly adjusting between consistency and availability. Custom Conflict Resolution Strategies Merge based on business semantics, such as using \u0026ldquo;union\u0026rdquo; for shopping carts, and using CRDT (G-counter, PN-counter, etc.) for counters to ensure data monotonicity. VII. Cross-Shard Strong Consistency Implementation in CP Systems # As mentioned in Chapter VII, database sharding can \u0026ldquo;split\u0026rdquo; the pressure of a single CP cluster into multiple sub-clusters to support higher concurrency. However, when a business needs to perform transactions across shards (i.e., involving updates to multiple databases or tables), it still faces the challenge of multi-shard consistency. The following are common approaches:\nDistributed Transactions: 2PC / 3PC If an application needs to perform atomic updates across multiple shards, distributed transaction protocols (such as 2PC, 3PC) are usually used to coordinate the commit or rollback of each shard. Problems and Countermeasures: 2PC/3PC both rely on a coordinator node, which can become a single point of failure. Blocking may occur in extreme cases of severe network partitions or coordinator failures. Generally, master-slave switching, heartbeat detection and timeout mechanisms, idempotent retries, and MVCC are used to reduce the impact of blocking and the risk of data inconsistency. Cell-based Architecture Divide the business into multiple autonomous units, with data in each unit within the same shard set, ensuring that most transactions are completed within a single unit, reducing cross-shard operations. Use asynchronous or eventual consistency mechanisms for data exchange at unit boundaries, taking into account overall high availability and consistency. Globally Distributed Database + Global Consensus Protocol For example, Google Spanner implements strong consistency replication of replicas through Paxos on each shard, and then uses the TrueTime API to provide global timestamps to ensure cross-shard consistency. This solution is extremely complex to implement, but can provide near-strong consistent distributed transaction capabilities on a global scale. Summary: For cross-shard transactions that strictly require strong consistency, 2PC/3PC + coordinator is still a common solution, and the probability of failure is reduced by maximizing the high availability of the coordinator. However, in engineering practice, cross-shard write operations should be minimized, or the complexity of the system should be reduced by using a cell-based approach to limit most transactions within a single shard.\nVIII. Discussion of Famous Cases # Here is a brief discussion of several distributed systems that are often mentioned in the industry, and their trade-offs and implementations in terms of CAP:\nGoogle BigTable A NoSQL database that leans towards CP, using distributed coordination between RegionServers and Master to ensure metadata consistency, and providing strong consistency guarantees for single-row transactions by default. Read consistency can be adjusted according to application requirements. Google Spanner A very powerful CP system (even achieving the \u0026ldquo;CA\u0026rdquo; illusion often spoken of, but still requiring some sacrifice in availability). A step further than BigTable, providing globally distributed SQL transactions and external consistency guarantees, supporting ACID transactions and read/write operations across global data centers. Uses TrueTime to provide high-precision external timestamps to ensure the order of transactions and external time consistency across data centers. Uses the Paxos protocol for data replication within each shard to ensure strong data consistency and high availability. Uses a highly available two-phase commit (2PC) protocol to implement reliable commit of global transactions, ensuring the consistency of cross-shard operations. Suitable for global financial transactions or other application scenarios with extremely high consistency requirements, but the infrastructure costs are higher. AWS DynamoDB A NoSQL database that leans towards AP, providing extremely high availability and eventual consistency. Provides weak consistent reads by default, but can obtain strong consistent reads through the ConsistentRead parameter. Provides DynamoDB Transaction, which supports ACID transactions, but performance will be reduced, and it only supports transactions within the same region data center. Cassandra Also AP leaning, using the Gossip protocol to maintain node topology status at the bottom layer. Consistency levels can be adjusted by parameters such as R and W. When R + W \u0026gt; N, strong consistency for single-row read/write is guaranteed. Supports Lightweight Transactions (LWT), providing single-row ACID transaction guarantees. Comparison shows: In engineering, there is no absolute \u0026ldquo;AP or CP\u0026rdquo;, but rather a mix of multiple consistency strategies; most systems provide a certain degree of tunable consistency to adapt to different application scenarios.\nIX. Conclusion # The CAP Theorem is not a one-size-fits-all solution Real distributed systems cannot simply say \u0026ldquo;I choose C, give up A\u0026rdquo; or \u0026ldquo;I choose A, give up C.\u0026rdquo; It is more common in the industry to flexibly choose the CP or AP mode for different data dimensions and different operation types. Even within the same system, different fault tolerance and consistency strategies are adopted for different tables/functions. AP is not absolutely 100% available For example, Cassandra, DynamoDB, etc., may also encounter situations where requests cannot be fulfilled during extreme network partitions or large-scale node failures. AP systems are designed to prefer \u0026ldquo;write as long as the replica is writable,\u0026rdquo; sacrificing some consistency guarantees in exchange for relatively higher availability and throughput. CP can also try to achieve high availability Paxos/Raft can also provide 99.99% or even higher availability under normal circumstances, but it requires more investment in network, hardware, and engineering costs. And in extreme network partitions, it will still block writes and sacrifice availability to maintain consistency. Hybrid architectures are mainstream Core transaction scenarios adhere to strong consistency (CP), while peripheral auxiliary scenarios or caching channels adopt weak consistency (AP), with the two working together. A comprehensive trade-off should be made based on business tolerance, network environment, cost investment, and team technical reserves. The CAP Theorem provides a high-level framework for thinking about the design of distributed systems, helping us make rational decisions in the face of the inevitable reality of network partitions. In real systems, richer consistency models, consensus protocols, multi-replica replication mechanisms, and engineering practices (disaster recovery, degradation, idempotency, conflict merging, etc.) are needed to balance consistency and availability.\n","date":"28 December 2024","externalUrl":null,"permalink":"/posts/1735350760948-cap/","section":"Posts","summary":"Discussing the Application of the CAP Theorem in Distributed Systems from Theory to Practice.","title":"In-depth explanation of the CAP theorem: Building high-concurrency and high-availability distributed systems","type":"posts"},{"content":"","date":"26 December 2024","externalUrl":null,"permalink":"/tags/algorithm/","section":"Tags","summary":"","title":"Algorithm","type":"tags"},{"content":"","date":"26 December 2024","externalUrl":null,"permalink":"/tags/divide-and-conquer-algorithm/","section":"Tags","summary":"","title":"Divide and Conquer Algorithm","type":"tags"},{"content":"","date":"26 December 2024","externalUrl":null,"permalink":"/tags/quick-sort/","section":"Tags","summary":"","title":"Quick Sort","type":"tags"},{"content":"Quick sort is a comparison-based unstable sorting algorithm that employs the divide-and-conquer strategy. It has an average time complexity of $O(n\\log n)$ and a worst-case time complexity of $O(n^2)$, with a space complexity of $O(1)$. The following will use sorting an integer sequence in ascending order as an example to introduce its implementation details and common mistakes.\nProblem Description # Given an integer sequence of length $n$, sort it in ascending order using quick sort and output the result.\nInput Format # The first line contains an integer $n$. The second line contains $n$ integers, all within the range $[1, 10^9]$. Output Format # Output the sorted sequence on a single line. Data Range # $1 \\leq n \\leq 100000$\nInput Example # 5 3 1 2 4 5 Output Example # 1 2 3 4 5 Quick Sort Idea # During each divide step in quick sort, a number is arbitrarily chosen as the pivot (the number in the middle position is chosen below).\nUse left and right pointers moving towards each other. The left pointer L searches from left to right for the first number greater than or equal to the pivot, and the right pointer R searches from right to left for the first number less than or equal to the pivot. Then, swap these two numbers.\nRepeat this process until the left and right pointers overlap or the left pointer is one position to the right of the right pointer. This is called one cycle.\nAfter each pointer movement and swap, ensure that the structure \u0026ldquo;left part ≤ pivot, right part ≥ pivot\u0026rdquo; is not broken, i.e., there is an invariant [left, L) \u0026lt;= pivot, (R, right] \u0026gt;= pivot.\nIn the example code below, left and right are the boundaries of the currently processed closed interval, and pivot is taken as the element at the midpoint of the interval.\n1#include \u0026lt;bits/stdc++.h\u0026gt; 2using namespace std; 3 4void quickSort(vector\u0026lt;int\u0026gt; \u0026amp;a, int left, int right) { 5 if (left \u0026gt;= right) return; 6 7 int pivot = a[(left + right) / 2]; 8 int l = left, r = right; 9 10 while (true) { 11 while (a[l] \u0026lt; pivot) l++; 12 while (a[r] \u0026gt; pivot) r--; 13 if (l \u0026gt;= r) break; 14 swap(a[l], a[r]); 15 l++; r--; 16 } 17 18 quickSort(a, left, r); 19 quickSort(a, r + 1, right); 20} 21 22int main() { 23 int n; cin \u0026gt;\u0026gt; n; 24 vector\u0026lt;int\u0026gt; a(n); 25 for (int i = 0; i \u0026lt; n; i++) cin \u0026gt;\u0026gt; a[i]; 26 27 quickSort(a, 0, n - 1); 28 29 for (int i = 0; i \u0026lt; n; i++) cout \u0026lt;\u0026lt; a[i] \u0026lt;\u0026lt; \u0026#34; \u0026#34;; 30 return 0; 31} Complexity and pivot Selection # Since quick sort has a worst-case complexity of $O(n^2)$, the selection of the pivot is crucial. If the first or last element is always chosen, the worst case is likely to occur in nearly sorted arrays.\nIn addition to taking the element in the middle position, a random element can also be selected as the pivot, or the median of the left, middle, and right elements can be taken as the pivot.\nCommon Error Examples # The following code contains several common errors.\n1#include \u0026lt;bits/stdc++.h\u0026gt; 2using namespace std; 3 4void quickSort(vector\u0026lt;int\u0026gt; \u0026amp;a, int left, int right) { 5 if (left == right) return; // 7 6 7 int pivot = (left + right) \u0026gt;\u0026gt; 1; // 1 8 int l = left, r = right; 9 10 while (true) { 11 while (a[l] \u0026lt;= pivot) l++; // 2 12 while (a[r] \u0026gt;= pivot) r--; // 2 13 swap(a[l], a[r]); 14 if (l \u0026gt;= r) break; // 3 15 // 4 16 } 17 18 quickSort(a, left, l - 1); // 5, 6 19 quickSort(a, l, right); // 5, 6 20} 21 22int main() { 23 int n; cin \u0026gt;\u0026gt; n; 24 vector\u0026lt;int\u0026gt; a(n); 25 for (int i = 0; i \u0026lt; n; i++) cin \u0026gt;\u0026gt; a[i]; 26 27 quickSort(a, 0, n - 1); 28 29 for (int i = 0; i \u0026lt; n; i++) cout \u0026lt;\u0026lt; a[i] \u0026lt;\u0026lt; \u0026#34; \u0026#34;; 30 31 return 0; 32} Error Analysis:\npivot should be a number in the array, not an index. Use \u0026lt; and \u0026gt; instead of \u0026lt;= and \u0026gt;=, otherwise the left pointer may move more than one position past the right pointer, and the array cannot be divided into two parts. After finding l \u0026gt;= r, the loop should be exited immediately, and no more swaps should be performed. Otherwise, it cannot be guaranteed that the elements on the left are no greater than pivot and the elements on the right are no less than pivot. After each swap, l++ and r-- should be executed. The pivot is actually taking the number in the middle-left position. Therefore, if using $l - 1$ and $l$ to divide the array, consider the array [1, 2]. It is easy to see that it will lead to an infinite loop, continuously dividing the array into two parts of size 0 and 2. Similarly, using $r$ and $l$ to divide the array will not work either. Instead, at the end of a cycle, $r$ must be less than $right$, so $r$ and $r+1$ can be used to divide the array. Readers can simulate the algorithm process to see why. Another simple way to avoid infinite loops is to randomly select the pivot or handle the case where there are only two elements specially. Also, using $l$, $l+1$ is not correct either, because this division does not conform to the definition. When $r$ is to the left of $l$, using $l$, $l+1$ cannot correctly divide the array into two parts where the left part is less than or equal to pivot and the right part is greater than or equal to pivot. This problem assumes that the array is not empty, so the case of \u0026gt; does not exist. However, it is recommended to use \u0026gt;=, which is safer. Supplement # Quick sort can also be evolved into \u0026ldquo;quick select\u0026rdquo;, which can find the $k$-th smallest number in an unordered array in $O(n)$ expected time. The specific idea is similar to quick sort, except that it only continues recursion in one sub-interval each time, thus reducing the time complexity.\n","date":"26 December 2024","externalUrl":null,"permalink":"/posts/1735252761946-quick-sort/","section":"Posts","summary":"Key Points for Correctly Implementing the Quick Sort Algorithm.","title":"Quick Sort","type":"posts"},{"content":"","date":"26 December 2024","externalUrl":null,"permalink":"/tags/sorting-algorithm/","section":"Tags","summary":"","title":"Sorting Algorithm","type":"tags"},{"content":"","date":"2024年12月26日","externalUrl":null,"permalink":"/zh-cn/tags/%E5%88%86%E6%B2%BB%E7%AE%97%E6%B3%95/","section":"Tags","summary":"","title":"分治算法","type":"tags"},{"content":"","date":"2024年12月26日","externalUrl":null,"permalink":"/zh-cn/tags/%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F/","section":"Tags","summary":"","title":"快速排序","type":"tags"},{"content":"","date":"2024年12月26日","externalUrl":null,"permalink":"/zh-cn/tags/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/","section":"Tags","summary":"","title":"排序算法","type":"tags"},{"content":"","date":"2024年12月26日","externalUrl":null,"permalink":"/zh-cn/tags/%E7%AE%97%E6%B3%95/","section":"Tags","summary":"","title":"算法","type":"tags"},{"content":" Problem # There are $N$ items. The volume of the $i$-th item is $s_i$, and its value is $v_i$. Each item can be taken only once. Under the premise of not exceeding the maximum total volume limit $S$, find the maximum total value $V$ that can be obtained.\nInput Format # The first line contains two integers, $N$ and $S$, separated by a space, representing the number of items and the maximum total volume limit, respectively. The following $N$ lines each contain two integers, $s_i$ and $v_i$, separated by a space, representing the volume and value of the $i$-th item, respectively.\nOutput Format # Output an integer representing the maximum value.\nData Range # $$0 \\le N, S \\leq 1000$$$$0 \\le s_i, v_i \\leq 1000$$ Input Example # 4 5 1 2 2 4 3 4 4 5 Output Example # 8 Solution # Define the state: f[i][j] represents the maximum value that can be obtained from the first $i$ items with a volume limit of $j$. If the $i$-th item is not taken, then f[i][j] = f[i - 1][j] If the $i$-th item is taken, then f[i][j] = f[i - 1][j - s[i]] + v[i] When implementing the state transition, pay attention to the domain range. If $j \u003c s_i$, then do not consider taking the $i$-th item. Because if $j - s_i$ is negative, the array index is invalid. It can also be explained as: the volume of the $i$-th item is greater than the volume limit, so it is impossible. Define the initial condition: For the first 0 items, any volume limit yields a value of 0, i.e., f[0][j] = 0, j $\\in [0, S]$. Time complexity: $O(NS)$. Code # 1#include\u0026lt;bits/stdc++.h\u0026gt; 2using namespace std; 3int main() { 4 int N, S; 5 cin \u0026gt;\u0026gt; N \u0026gt;\u0026gt; S; 6 vector\u0026lt;int\u0026gt; s(N + 1), v(N + 1); 7 for (int i = 1; i \u0026lt;= N; i++) cin \u0026gt;\u0026gt; s[i] \u0026gt;\u0026gt; v[i]; 8 vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; f(N + 1, vector\u0026lt;int\u0026gt;(S + 1)); 9 for (int i = 1; i \u0026lt;= N; i++) { 10 for (int j = 0; j \u0026lt;= S; j++) { 11 f[i][j] = f[i - 1][j]; 12 if (j \u0026gt;= s[i]) f[i][j] = max(f[i][j], f[i - 1][j - s[i]] + v[i]); 13 } 14 } 15 cout \u0026lt;\u0026lt; f[N][S] \u0026lt;\u0026lt; endl; 16 return 0; 17} 1D DP Optimization # Compressing the two-dimensional array into a one-dimensional array can significantly save space and improve the running speed to a certain extent (the disadvantage is that it cannot meet the special requirements of some problem types). Note that in the state transition, f[i][j] is only related to f[i - 1][j] and f[i - 1][j - s[i]]. In other words, in the two-dimensional array f in the code, f[i][j] is only related to the elements in the previous row that are to its left or in the same column. Therefore, the two-dimensional array can be compressed into a one-dimensional array or a rolling array. Note that in the code below, the second loop iterates in reverse order. This is because we need to ensure that when calculating f[i][j], f[i - 1][j - s[i]] has not been updated yet. 1#include\u0026lt;bits/stdc++.h\u0026gt; 2using namespace std; 3int main() { 4 int N, S; 5 cin \u0026gt;\u0026gt; N \u0026gt;\u0026gt; S; 6 vector\u0026lt;int\u0026gt; s(N + 1), v(N + 1); 7 for (int i = 1; i \u0026lt;= N; i++) cin \u0026gt;\u0026gt; s[i] \u0026gt;\u0026gt; v[i]; 8 vector\u0026lt;int\u0026gt; f(S + 1); 9 for (int i = 1; i \u0026lt;= N; i++) { 10 for (int j = S; j \u0026gt;= s[i]; j--) { 11 f[j] = max(f[j], f[j - s[i]] + v[i]); 12 } 13 } 14 cout \u0026lt;\u0026lt; f[S] \u0026lt;\u0026lt; endl; 15 return 0; 16} If the Number of Schemes is Required # Not only should the maximum total value that can be obtained be output, but also \u0026ldquo;how many different selection methods can achieve this maximum total value\u0026rdquo;. The following introduces how to count the number of schemes in the 01 knapsack problem.\n2D DP for Counting Schemes # The following uses 2D DP as an example for explanation.\nDefine the state:\ndp[i][j] represents \u0026ldquo;the maximum value that can be obtained when considering the first i items with a capacity ( volume limit) of j\u0026rdquo;. ways[i][j] represents \u0026ldquo;the number of schemes corresponding to the maximum value obtained when considering the first i items with a capacity of j\u0026rdquo;. State transition:\nIf the $i$-th item is not selected: $$ \\text{dp}[i][j] = \\text{dp}[i-1][j], \\quad \\text{ways}[i][j] = \\text{ways}[i-1][j] $$ If the $i$-th item is selected (provided that $ j \\ge s_i $): $$ \\text{dp}[i][j] = \\text{dp}[i-1][j - s_i] + v_i, \\quad \\text{ways}[i][j] = \\text{ways}[i-1][j - s_i] $$ Whether to select or not, the final dp[i][j] should take the larger of the two: If $$ \\text{dp}[i-1][j - s_i] + v_i $$ then it means that \"selecting the i-th item\" has a greater value: $$ \\text{dp}[i][j] = \\text{dp}[i-1][j - s_i] + v_i, \\quad \\text{ways}[i][j] = \\text{ways}[i-1][j - s_i]. $$\nIf $$ \\text{dp}[i-1][j - s_i] + v_i = \\text{dp}[i-1][j], $$ it means that the maximum value obtained by the two methods is the same, then the number of schemes should be added: $$ \\text{dp}[i][j] = \\text{dp}[i-1][j], \\quad \\text{ways}[i][j] = \\text{ways}[i-1][j] \\text{ways}[i-1][j - s_i]. $$ If $$ \\text{dp}[i-1][j - s_i] + v_i \u003c \\text{dp}[i-1][j], $$ then it means that \u0026ldquo;not selecting the i-th item\u0026rdquo; has a greater value, and the number of schemes inherits the number of schemes when not selecting: $$ \\text{dp}[i][j] = \\text{dp}[i-1][j], \\quad \\text{ways}[i][j] = \\text{ways}[i-1][j]. $$ Initial conditions:\ndp[0][j] = 0 means that when there are 0 items, the maximum value obtained for any capacity is 0. ways[0][0] = 1 means that \u0026ldquo;0 items, capacity 0\u0026rdquo; is a feasible scheme (i.e., selecting nothing), and the number of schemes is set to 1. For j \u0026gt; 0, when there are no items to choose from and the capacity is greater than 0, it is impossible to obtain any positive value, and the corresponding number of schemes is 0, i.e., ways[0][j] = 0. Final answer:\ndp[N][S] is the maximum value. ways[N][S] is the number of schemes to achieve this maximum value. Time complexity: $O(NS)$. This problem can also be optimized using 1D DP. If the Requirement is to Exactly Reach the Volume Limit # Define the state: f[i][j] represents the maximum value when the first i items have exactly a volume of $j$. If the $i$-th item is not taken, then f[i][j] = f[i - 1][j] If the $i$-th item is taken, then f[i][j] = f[i - 1][j - s[i]] + v[i] It can be noted that there is no difference in the state transition from the original problem. However, the initial conditions are different. Besides f[0][0] = 0, the rest f[0][j] = $-\\infty$, j $\\in [1, S]$. $-\\infty$ represents an impossible state. If the Volume Limit $S$ is Very Large (1e9), While the Number of Items $N$ and the Maximum Total Value $V$ are Relatively Small # For such problems, there is a solution with a complexity of $O(NV)$. Define the state: f[i][j] represents the minimum volume when selecting several items from the first i items, with a total value of exactly j. If the $i$-th item is not taken, then f[i][j] = f[i - 1][j] If the $i$-th item is taken, then f[i][j] = f[i - 1][j - v[i]] + s[i] Take the smaller of the two. Initial conditions: f[0][0] = 0, the rest f[0][j] = $\\infty$, j $\\in [1, V]$. $\\infty$ represents an impossible state. Note that it is not $-\\infty$. The final answer is the largest j in f[N][j] such that f[N][j] \u0026lt;= S. If the Volume Limit $S$ and the Value of a Single Item $v_i$ are Both Very Large (on the order of 1e9), While the Number of Items $N$ is Very Small (no more than 40 at most) # When $N \\leq 20$, all subsets can be directly enumerated by brute force (time complexity $O(2^N)$). When $N \\leq 40$, since $2^{40}$ is on the order of $10^{12}$, direct brute force will also be relatively large, so * meet-in-the-middle search* can be used to reduce the complexity to approximately $O\\bigl(2^{\\frac{N}{2}} \\times \\log(2^{\\frac{N}{2}})\\bigr) \\approx O(N \\cdot 2^{\\frac{N}{2}})$, which can be completed in an acceptable time. ","date":"24 December 2024","externalUrl":null,"permalink":"/posts/1735112650761-0-1-knapsack-problem/","section":"Posts","summary":"The most basic classic knapsack problem.","title":"0-1 knapsack problem","type":"posts"},{"content":"","date":"24 December 2024","externalUrl":null,"permalink":"/tags/binary-search/","section":"Tags","summary":"","title":"Binary Search","type":"tags"},{"content":"If an ordered solution space is divided into left and right parts, where one part satisfies a condition and the other does not, then binary search can be used to find the critical point in the ordered solution space.\nThe basic idea of binary search is to repeatedly halve the search interval. Each time, the middle element is checked. If the middle element does not satisfy the condition, half of the interval can be excluded; otherwise, the search continues in the other half. Since half of the search interval is discarded each time, the search time complexity can reach $O(\\log n)$.\nExample Problem # Problem Description: Given an ascendingly sorted integer array of length $n$, and $q$ queries. Each query gives an integer $k$, and we need to find the \u0026ldquo;starting position\u0026rdquo; and \u0026ldquo;ending position\u0026rdquo; of $k$ in the array (indices start from 0). If the number does not exist in the array, return -1 -1.\nInput Format # First line: two integers $n$ and $q$, representing the length of the array and the number of queries, respectively. Second line: $n$ integers, representing the complete array, already sorted in ascending order. Next $q$ lines: each line contains an integer $k$, representing a query element. Data Range # $1 \\leq n \\leq 100000$\n$1 \\leq q \\leq 10000$\n$1 \\leq k \\leq 10000$\nOutput Format # For each query, output the starting and ending positions of the element in the array on one line. If the element does not exist in the array, output -1 -1.\nExample:\nInput: 6 3 1 2 2 3 3 4 3 4 5 Output: 3 4 5 5 -1 -1 Explanation:\nThe range where element $3$ appears is $[3, 4]$; Element $4$ appears only once, at position $5$; Element $5$ does not exist in the array, so return $-1$ $-1$. Solution # Finding the \u0026ldquo;Starting Position\u0026rdquo;: That is, finding the first position greater than or equal to $k$. The array can be divided into two parts:\nAll numbers on the left are \u0026ldquo;less than\u0026rdquo; $k$ All numbers on the right are \u0026ldquo;greater than or equal to\u0026rdquo; $k$ The answer is the first position on the right Finding the \u0026ldquo;Ending Position\u0026rdquo;: That is, finding the last position less than or equal to $k$. The array can be divided into two parts:\nAll numbers on the left are \u0026ldquo;less than or equal to\u0026rdquo; $k$ All numbers on the right are \u0026ldquo;greater than\u0026rdquo; $k$ The answer is the last position on the left Recommended Template # Below is an elegant and less error-prone binary search template.\nDefine two pointers $l, r$, with the invariant: the closed interval $[0, l]$ all belongs to the left part, and the closed interval $[r, n - 1]$ all belongs to the right part. $l$ and $r$ are initialized to $-1$ and $n$, respectively.\nWhen the algorithm terminates, $l$ and $r$ are adjacent, pointing to the last element of the left part and the first element of the right part, respectively.\nBecause the solution we want may not exist, if the problem does not state that a solution definitely exists, we need to check if l or r is out of bounds and if it points to the correct value.\n1#include \u0026lt;bits/stdc++.h\u0026gt; 2using namespace std; 3 4int main() { 5 int n, q; 6 cin \u0026gt;\u0026gt; n \u0026gt;\u0026gt; q; 7 vector\u0026lt;int\u0026gt; nums(n); 8 for(int i = 0; i \u0026lt; n; i++) cin \u0026gt;\u0026gt; nums[i]; 9 10 while(q--) { 11 int k; 12 cin \u0026gt;\u0026gt; k; 13 14 // 1. Find the starting position of k 15 // Divide the array into two parts, the left part is all \u0026lt; k, and the right part is all \u0026gt;= k. 16 // The answer is the smallest index of the right part. 17 int l = -1, r = n; 18 while(l \u0026lt; r - 1) { 19 int mid = (l + r) / 2; 20 if(nums[mid] \u0026gt;= k) r = mid; 21 else l = mid; 22 } 23 24 // If r is out of bounds or nums[r] != k, it means k does not exist 25 if (r == n || nums[r] != k) { 26 cout \u0026lt;\u0026lt; -1 \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; -1 \u0026lt;\u0026lt; endl; 27 continue; 28 } 29 30 int leftPos = r; 31 32 // 2. Find the ending position of k 33 // Divide the array into two parts, the left part is all \u0026lt;= k, and the right part is all \u0026gt; k. 34 // The answer is the largest index of the left part. 35 l = -1, r = n; 36 while(l \u0026lt; r - 1) { 37 int mid = (l + r) / 2; 38 if(nums[mid] \u0026lt;= k) l = mid; 39 else r = mid; 40 } 41 42 int rightPos = l; 43 cout \u0026lt;\u0026lt; leftPos \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; rightPos \u0026lt;\u0026lt; endl; 44 } 45 46 return 0; 47} Advantages # This approach has strictly defined invariants. It applies to both finding the \u0026ldquo;starting position\u0026rdquo; and the \u0026ldquo;ending position\u0026rdquo; without extra handling or changes. Some approaches use l == r as the termination condition. When $l$ and $r$ differ by $1$, it will calculate $mid$ equal to $l$ or $r$. If not handled correctly, updating $l$ or $r$ to $mid$ will not shrink the search interval, leading to an infinite loop. In contrast, this approach terminates when $l$ and $r$ are adjacent, ensuring that $mid$ is less than $l$ and greater than $r$, and updating $l$ or $r$ will always shrink the search interval. STL # If you use the lower_bound and upper_bound functions provided by C++ STL, you can also accomplish the same thing:\nlower_bound(first, last, val) will return \u0026ldquo;the first position greater than or equal to val\u0026rdquo; upper_bound(first, last, val) will return \u0026ldquo;the first position greater than val\u0026rdquo; For example, suppose nums = {1,2,3,4,4,4,4,4,5,5,6}, and we want to know the interval where 4 appears:\n1vector\u0026lt;int\u0026gt; nums = {1,2,3,4,4,4,4,4,5,5,6}; 2auto it1 = lower_bound(nums.begin(), nums.end(), 4); 3auto it2 = upper_bound(nums.begin(), nums.end(), 4); 4 5if (it1 == nums.end() || *it1 != 4) { 6 cout \u0026lt;\u0026lt; \u0026#34;4 appears 0 times\u0026#34; \u0026lt;\u0026lt; endl; 7} else { 8 cout \u0026lt;\u0026lt; \u0026#34;first 4 is at \u0026#34; \u0026lt;\u0026lt; it1 - nums.begin() \u0026lt;\u0026lt; endl; 9 cout \u0026lt;\u0026lt; \u0026#34;last 4 is at \u0026#34; \u0026lt;\u0026lt; it2 - nums.begin() - 1 \u0026lt;\u0026lt; endl; 10 cout \u0026lt;\u0026lt; \u0026#34;4 appears \u0026#34; \u0026lt;\u0026lt; it2 - it1 \u0026lt;\u0026lt; \u0026#34; times\u0026#34; \u0026lt;\u0026lt; endl; 11} it1 points to the first position where the value is greater than or equal to $4$. it2 points to the first position where the value is greater than $4$. Therefore, it2 - it1 is the number of times $4$ appears in the array; it2 - nums.begin() - 1 is the position of the right boundary of $4$. Supplement # Binary search can also be extended to search in floating-point ranges (such as finding the roots of an equation), and ternary search to find the extrema of unimodal functions.\nPractice # LeetCode 33. Search in Rotated Sorted Array\nHint: First, use binary search to find the rotation point, and then use binary search to find the target value.\n","date":"24 December 2024","externalUrl":null,"permalink":"/posts/1735078223417-binary-search-intro/","section":"Posts","summary":"How to implement the binary search algorithm elegantly.","title":"Binary Search","type":"posts"},{"content":"","date":"24 December 2024","externalUrl":null,"permalink":"/series/classic-knapsack-problem-set/","section":"Series","summary":"","title":"Classic Knapsack Problem Set","type":"series"},{"content":"","date":"24 December 2024","externalUrl":null,"permalink":"/tags/dynamic-programming/","section":"Tags","summary":"","title":"Dynamic Programming","type":"tags"},{"content":"","date":"24 December 2024","externalUrl":null,"permalink":"/tags/knapsack-problem/","section":"Tags","summary":"","title":"Knapsack Problem","type":"tags"},{"content":"","date":"24 December 2024","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","date":"2024年12月24日","externalUrl":null,"permalink":"/zh-cn/tags/%E4%BA%8C%E5%88%86%E6%90%9C%E7%B4%A2/","section":"Tags","summary":"","title":"二分搜索","type":"tags"},{"content":"","date":"2024年12月24日","externalUrl":null,"permalink":"/zh-cn/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/","section":"Tags","summary":"","title":"动态规划","type":"tags"},{"content":"","date":"2024年12月24日","externalUrl":null,"permalink":"/zh-cn/series/%E8%83%8C%E5%8C%85%E7%BB%8F%E5%85%B8%E9%97%AE%E9%A2%98%E9%9B%86/","section":"Series","summary":"","title":"背包经典问题集","type":"series"},{"content":"","date":"2024年12月24日","externalUrl":null,"permalink":"/zh-cn/tags/%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98/","section":"Tags","summary":"","title":"背包问题","type":"tags"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"}]